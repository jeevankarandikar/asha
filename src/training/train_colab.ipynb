{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Asha - Image â†’ MANO Î¸ Training Pipeline\n",
    "\n",
    "This notebook trains a neural network to predict MANO parameters directly from images, bypassing MediaPipe + IK optimization.\n",
    "\n",
    "**Architectures:**\n",
    "- ResNet + MLP: Fast baseline (~15-20mm target)\n",
    "- Transformer: Better accuracy (~12-15mm target, HandFormer-style)\n",
    "\n",
    "**Dataset:** FreiHAND (32K poses Ã— 4 views = 130K images)\n",
    "\n",
    "**Setup:** All code is inline. Clone repo, mount Drive, copy dataset, train, save to Drive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Working directory: /content\n",
      "âœ“ Repo root: /content\n",
      "âœ“ MANO models: /content/models\n",
      "âœ“ Checkpoints: /content/checkpoints\n"
     ]
    }
   ],
   "source": [
    "# clone repository from github\n",
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "repo_url = \"https://github.com/jeevankarandikar/asha.git\"\n",
    "repo_dir = \"/content/asha\"\n",
    "\n",
    "if not os.path.exists(repo_dir):\n",
    "    print(f\"cloning repository from github...\")\n",
    "    subprocess.run(['git', 'clone', repo_url, repo_dir], check=True)\n",
    "    print(\"âœ“ repository cloned\")\n",
    "else:\n",
    "    print(f\"âœ“ repository already exists at {repo_dir}\")\n",
    "\n",
    "os.chdir(repo_dir)\n",
    "\n",
    "# store paths globally\n",
    "REPO_ROOT = Path(repo_dir)\n",
    "MANO_ROOT = REPO_ROOT / 'models'\n",
    "CHECKPOINT_DIR_LOCAL = REPO_ROOT / 'checkpoints'\n",
    "\n",
    "print(f\"\\nâœ“ working directory: {os.getcwd()}\")\n",
    "print(f\"âœ“ repo root: {REPO_ROOT}\")\n",
    "print(f\"âœ“ mano models: {MANO_ROOT}\")\n",
    "print(f\"âœ“ local checkpoints: {CHECKPOINT_DIR_LOCAL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Dependencies installed\n",
      "âœ“ All imports loaded\n"
     ]
    }
   ],
   "source": [
    "# install dependencies\n",
    "import subprocess\n",
    "\n",
    "# colab comes with torch/torchvision, but we ensure cuda version and install missing packages\n",
    "packages = [\n",
    "    'torch',  # colab has this but we ensure latest\n",
    "    'torchvision',  # colab has this but we ensure latest\n",
    "    'albumentations',  # data augmentation\n",
    "    'tqdm',  # progress bars\n",
    "    'tensorboard',  # logging\n",
    "    'opencv-python',  # image loading (colab has this but we ensure latest)\n",
    "    'scipy',  # needed for sparse matrices in mano model\n",
    "    'matplotlib',  # plotting (colab has this but we ensure latest)\n",
    "    'h5py',  # dataset loading\n",
    "]\n",
    "\n",
    "subprocess.run(['pip', 'install'] + packages + ['-q'], check=True)\n",
    "print(\"âœ“ dependencies installed\")\n",
    "\n",
    "# import all required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pickle\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from datetime import datetime\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse\n",
    "\n",
    "print(\"âœ“ all imports loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SYSTEM SPECIFICATIONS\n",
      "============================================================\n",
      "\n",
      "âš ï¸  No GPU detected (running on CPU)\n",
      "  Training will be very slow!\n",
      "\n",
      "ðŸ”§ PyTorch:\n",
      "  CUDA Available: False\n",
      "  âš ï¸  Running on CPU - training will be very slow!\n",
      "  ðŸ’¡ Tip: Use Google Colab for free GPU access\n",
      "\n",
      "ðŸ’» System:\n",
      "  Platform: Linux\n",
      "  Python: 3.12.12\n",
      "  PyTorch: 2.9.0+cu126\n",
      "  CPU Cores: 2\n",
      "\n",
      "ðŸŒ Environment: Google Colab\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Check GPU and system specs (works in both Colab and local)\n",
    "import torch\n",
    "import platform\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SYSTEM SPECIFICATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# GPU Info - Works in both Colab and local\n",
    "is_colab = 'COLAB_GPU' in os.environ or 'google.colab' in str(os.environ)\n",
    "\n",
    "try:\n",
    "    # Suppress stderr to avoid \"command not found\" messages\n",
    "    result = subprocess.run(\n",
    "        ['nvidia-smi'], \n",
    "        capture_output=True, \n",
    "        text=True, \n",
    "        timeout=5,\n",
    "        stderr=subprocess.DEVNULL  # Suppress error messages\n",
    "    )\n",
    "    if result.returncode == 0 and result.stdout:\n",
    "        env_name = 'Colab' if is_colab else 'Local'\n",
    "        print(f'\\nâœ… GPU Connected ({env_name}):')\n",
    "        print(result.stdout)\n",
    "    else:\n",
    "        raise FileNotFoundError(\"nvidia-smi not available\")\n",
    "except (FileNotFoundError, subprocess.TimeoutExpired):\n",
    "    print('\\nâš ï¸  No GPU detected (running on CPU)')\n",
    "    print('  Training will be very slow!')\n",
    "    if not is_colab:\n",
    "        print('  ðŸ’¡ Tip: Use Google Colab for free GPU access')\n",
    "except Exception as e:\n",
    "    print(f'\\nâš ï¸  Could not check GPU status: {e}')\n",
    "    print('  Assuming CPU mode')\n",
    "\n",
    "# PyTorch GPU check\n",
    "print(f\"\\nðŸ”§ PyTorch:\")\n",
    "print(f\"  CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"  âš ï¸  Running on CPU - training will be very slow!\")\n",
    "    print(\"  ðŸ’¡ Tip: Use Google Colab for free GPU access\")\n",
    "\n",
    "# System Info\n",
    "print(f\"\\nðŸ’» System:\")\n",
    "print(f\"  Platform: {platform.system()}\")\n",
    "print(f\"  Python: {platform.python_version()}\")\n",
    "print(f\"  PyTorch: {torch.__version__}\")\n",
    "\n",
    "try:\n",
    "    import multiprocessing\n",
    "    print(f\"  CPU Cores: {multiprocessing.cpu_count()}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Environment info\n",
    "if is_colab:\n",
    "    print(f\"\\nðŸŒ Environment: Google Colab\")\n",
    "else:\n",
    "    print(f\"\\nðŸŒ Environment: Local ({'CPU' if not torch.cuda.is_available() else 'GPU'})\")\n",
    "\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive & Copy Dataset\n",
    "\n",
    "**Steps:**\n",
    "1. Mount Google Drive (cell below)\n",
    "2. Copy dataset from Drive to Colab\n",
    "3. Dataset should be at: `MyDrive/datasets/freihand/` or `MyDrive/freihand/`\n",
    "\n",
    "**If you haven't uploaded yet:**\n",
    "- Upload your `freihand` folder from `~/datasets/freihand` to Google Drive\n",
    "- Place it in `MyDrive/datasets/freihand/` or `MyDrive/freihand/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mount google drive and extract dataset zip\n",
    "from google.colab import drive\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"mounting google drive...\")\n",
    "drive.mount('/content/drive')\n",
    "print(\"âœ“ google drive mounted\")\n",
    "\n",
    "# find zip file in drive\n",
    "drive_zip_paths = [\n",
    "    Path('/content/drive/MyDrive/datasets/freihand.zip'),\n",
    "    Path('/content/drive/MyDrive/freihand.zip'),\n",
    "]\n",
    "\n",
    "drive_zip_path = None\n",
    "for path in drive_zip_paths:\n",
    "    if path.exists():\n",
    "        drive_zip_path = path\n",
    "        print(f\"âœ“ dataset zip found: {path}\")\n",
    "        break\n",
    "\n",
    "if not drive_zip_path:\n",
    "    print(\"\\nâŒ dataset zip not found in drive\")\n",
    "    print(\"   upload freihand.zip to mydrive/datasets/\")\n",
    "    raise FileNotFoundError(\"dataset zip not found in google drive\")\n",
    "\n",
    "# extract zip file to colab\n",
    "local_dataset_path = Path('/content/freihand')\n",
    "if local_dataset_path.exists():\n",
    "    print(f\"\\nâš ï¸  dataset already exists at {local_dataset_path}\")\n",
    "    print(\"   delete it first if you want to re-extract\")\n",
    "else:\n",
    "    print(f\"\\nextracting zip file to colab...\")\n",
    "    print(f\"   this may take 5-10 minutes for ~3.7gb...\")\n",
    "    with zipfile.ZipFile(drive_zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall('/content/')\n",
    "    print(\"âœ“ dataset extracted\")\n",
    "\n",
    "# verify dataset\n",
    "required_files = [\"training_mano.json\", \"training_xyz.json\", \"training/rgb\"]\n",
    "missing = [f for f in required_files if not Path(local_dataset_path, f).exists()]\n",
    "\n",
    "if missing:\n",
    "    print(f\"\\nâŒ missing files: {missing}\")\n",
    "    raise FileNotFoundError(\"dataset incomplete\")\n",
    "else:\n",
    "    print(f\"\\nâœ“ dataset verified\")\n",
    "    num_images = len(list((local_dataset_path / \"training\" / \"rgb\").glob(\"*.jpg\")))\n",
    "    print(f\"âœ“ found {num_images:,} images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Configuration\n",
    "\n",
    "Configure your training parameters here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training configuration\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# dataset path (should be /content/freihand after copying from drive)\n",
    "dataset_path = '/content/freihand'\n",
    "if not os.path.exists(dataset_path):\n",
    "    print(f\"âš ï¸  dataset not found at {dataset_path}\")\n",
    "    print(\"   make sure you ran the drive mount cell above\")\n",
    "else:\n",
    "    print(f\"âœ“ dataset path: {dataset_path}\")\n",
    "\n",
    "# setup checkpoint directories\n",
    "CHECKPOINT_DIR_DRIVE = Path('/content/drive/MyDrive/asha_checkpoints')\n",
    "CHECKPOINT_DIR_LOCAL = Path('/content/asha/checkpoints')\n",
    "\n",
    "# Create directories\n",
    "CHECKPOINT_DIR_DRIVE.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_DIR_LOCAL.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"âœ“ Checkpoints will be saved to:\")\n",
    "print(f\"  - Google Drive: {CHECKPOINT_DIR_DRIVE}\")\n",
    "print(f\"  - Local backup: {CHECKPOINT_DIR_LOCAL}\")\n",
    "\n",
    "config = {\n",
    "    # dataset\n",
    "    'dataset_path': dataset_path,\n",
    "    'batch_size': 64,  # a100 can handle 64-128, reduce if oom\n",
    "    'num_workers': 8,  # more workers for a100\n",
    "    'image_size': 224,\n",
    "    'use_all_views': True,  # use all 4 views per pose (130k samples) or false (32k samples)\n",
    "    'train_split': 0.9,  # 90% train, 10% val\n",
    "    \n",
    "    # model\n",
    "    'architecture': 'resnet',  # 'resnet' or 'transformer'\n",
    "    'backbone': 'resnet50',  # 'resnet50' or 'resnet101'\n",
    "    'pretrained': True,\n",
    "    'predict_beta': True,\n",
    "    \n",
    "    # training\n",
    "    'epochs': 50,\n",
    "    'lr': 1e-4,\n",
    "    'weight_decay': 1e-5,\n",
    "    'lr_schedule': 'cosine',  # 'cosine', 'step', or 'none'\n",
    "    'use_mixed_precision': True,  # use fp16 for faster training on a100\n",
    "    \n",
    "    # loss\n",
    "    'use_mano_loss': False,  # true = slower but more accurate (requires mano forward pass)\n",
    "    'lambda_param': 1.0,\n",
    "    'lambda_joint': 10.0,\n",
    "    'lambda_reg': 0.01,\n",
    "    \n",
    "    # output\n",
    "    'output_dir_drive': str(CHECKPOINT_DIR_DRIVE),  # primary: google drive\n",
    "    'output_dir_local': str(CHECKPOINT_DIR_LOCAL),  # backup: local\n",
    "    'save_periodic_checkpoints': False,  # set true to save every 10 epochs (adds ~1-2gb per experiment)\n",
    "    \n",
    "    # system\n",
    "    'gpu': 0,\n",
    "}\n",
    "\n",
    "print(\"\\nconfiguration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define All Training Code (Inline)\n",
    "\n",
    "All code is included here - dataset loader, models, losses, and MANO layer. No external files needed!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Models (ResNet & Transformer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODEL ARCHITECTURES\n",
    "# ============================================================================\n",
    "\n",
    "class ResNetMANO(nn.Module):\n",
    "    \"\"\"resnet backbone + mlp head for mano parameter prediction.\"\"\"\n",
    "    \n",
    "    def __init__(self, backbone: str = \"resnet50\", pretrained: bool = True,\n",
    "                 predict_beta: bool = True, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # backbone\n",
    "        if backbone == \"resnet50\":\n",
    "            resnet = models.resnet50(pretrained=pretrained)\n",
    "            feature_dim = 2048\n",
    "        elif backbone == \"resnet101\":\n",
    "            resnet = models.resnet101(pretrained=pretrained)\n",
    "            feature_dim = 2048\n",
    "        else:\n",
    "            raise ValueError(f\"unknown backbone: {backbone}\")\n",
    "        \n",
    "        # remove final fc layer\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        \n",
    "        # mlp head for mano parameters\n",
    "        self.predict_beta = predict_beta\n",
    "        \n",
    "        # pose parameters Î¸ (45 dof)\n",
    "        self.theta_head = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 45),\n",
    "        )\n",
    "        \n",
    "        # shape parameters Î² (10 dof, optional)\n",
    "        if predict_beta:\n",
    "            self.beta_head = nn.Sequential(\n",
    "                nn.Linear(feature_dim, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(256, 10),\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # extract features\n",
    "        features = self.backbone(x)  # [B, 2048, 1, 1]\n",
    "        features = features.view(features.size(0), -1)  # [B, 2048]\n",
    "        \n",
    "        # predict mano parameters\n",
    "        theta = self.theta_head(features)  # [B, 45]\n",
    "        \n",
    "        if self.predict_beta:\n",
    "            beta = self.beta_head(features)  # [B, 10]\n",
    "            return theta, beta\n",
    "        else:\n",
    "            return theta\n",
    "\n",
    "\n",
    "class HandFormer(nn.Module):\n",
    "    \"\"\"transformer-based architecture inspired by handformer (2024).\"\"\"\n",
    "    \n",
    "    def __init__(self, backbone: str = \"resnet50\", pretrained: bool = True,\n",
    "                 d_model: int = 256, nhead: int = 8, num_layers: int = 4,\n",
    "                 dim_feedforward: int = 1024, dropout: float = 0.1, predict_beta: bool = True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # backbone\n",
    "        if backbone == \"resnet50\":\n",
    "            resnet = models.resnet50(pretrained=pretrained)\n",
    "            feature_dim = 2048\n",
    "        else:\n",
    "            raise ValueError(f\"unknown backbone: {backbone}\")\n",
    "        \n",
    "        # remove final layers, keep up to avgpool\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        \n",
    "        # feature projection\n",
    "        self.feature_proj = nn.Conv2d(feature_dim, d_model, kernel_size=1)\n",
    "        \n",
    "        # positional encoding (learned)\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(1, 196, d_model))  # 14x14 = 196\n",
    "        \n",
    "        # transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout, batch_first=True,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # global pooling\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        # mlp heads\n",
    "        self.predict_beta = predict_beta\n",
    "        \n",
    "        self.theta_head = nn.Sequential(\n",
    "            nn.Linear(d_model, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 45),\n",
    "        )\n",
    "        \n",
    "        if predict_beta:\n",
    "            self.beta_head = nn.Sequential(\n",
    "                nn.Linear(d_model, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(256, 10),\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        \n",
    "        # extract features\n",
    "        features = self.backbone(x)  # [B, 2048, H', W']\n",
    "        features = self.feature_proj(features)  # [B, d_model, H', W']\n",
    "        \n",
    "        # flatten spatial dimensions\n",
    "        B, C, H, W = features.shape\n",
    "        features = features.view(B, C, H * W).permute(0, 2, 1)  # [B, H*W, d_model]\n",
    "        \n",
    "        # add positional encoding\n",
    "        features = features + self.pos_encoding\n",
    "        \n",
    "        # transformer encoding\n",
    "        encoded = self.transformer(features)  # [B, H*W, d_model]\n",
    "        \n",
    "        # global pooling\n",
    "        pooled = self.global_pool(encoded.permute(0, 2, 1)).squeeze(-1)  # [B, d_model]\n",
    "        \n",
    "        # predict mano parameters\n",
    "        theta = self.theta_head(pooled)  # [B, 45]\n",
    "        \n",
    "        if self.predict_beta:\n",
    "            beta = self.beta_head(pooled)  # [B, 10]\n",
    "            return theta, beta\n",
    "        else:\n",
    "            return theta\n",
    "\n",
    "\n",
    "def create_model(architecture: str = \"resnet\", **kwargs) -> nn.Module:\n",
    "    \"\"\"factory function to create model.\"\"\"\n",
    "    if architecture == \"resnet\":\n",
    "        return ResNetMANO(**kwargs)\n",
    "    elif architecture == \"transformer\":\n",
    "        return HandFormer(**kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown architecture: {architecture}\")\n",
    "\n",
    "print(\"âœ“ Models defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LOSS FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "class SimpleMANOLoss(nn.Module):\n",
    "    \"\"\"Simplified loss: just parameter L2 + regularization. Fast, no MANO forward pass.\"\"\"\n",
    "    \n",
    "    def __init__(self, lambda_param: float = 1.0, lambda_reg: float = 0.01):\n",
    "        super().__init__()\n",
    "        self.lambda_param = lambda_param\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "    \n",
    "    def forward(self, pred_theta: torch.Tensor, pred_beta: torch.Tensor,\n",
    "                gt_theta: torch.Tensor, gt_beta: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        # Parameter loss\n",
    "        loss_param_theta = self.mse_loss(pred_theta, gt_theta)\n",
    "        loss_param_beta = self.mse_loss(pred_beta, gt_beta)\n",
    "        loss_param = loss_param_theta + 0.1 * loss_param_beta\n",
    "        \n",
    "        # Regularization\n",
    "        loss_reg = (pred_theta ** 2).mean() + (pred_beta ** 2).mean()\n",
    "        \n",
    "        # Total\n",
    "        total_loss = self.lambda_param * loss_param + self.lambda_reg * loss_reg\n",
    "        \n",
    "        return {\n",
    "            'param': loss_param,\n",
    "            'reg': loss_reg,\n",
    "            'total': total_loss,\n",
    "        }\n",
    "\n",
    "\n",
    "class MANOLoss(nn.Module):\n",
    "    \"\"\"Multi-term loss: parameter + joint position + regularization. Slower but more accurate.\"\"\"\n",
    "    \n",
    "    def __init__(self, lambda_param: float = 1.0, lambda_joint: float = 10.0,\n",
    "                 lambda_vertex: float = 1.0, lambda_reg: float = 0.01,\n",
    "                 use_vertex_loss: bool = False, mano_layer: Optional[object] = None):\n",
    "        super().__init__()\n",
    "        self.lambda_param = lambda_param\n",
    "        self.lambda_joint = lambda_joint\n",
    "        self.lambda_vertex = lambda_vertex\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.use_vertex_loss = use_vertex_loss\n",
    "        self.mano_layer = mano_layer\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "    \n",
    "    def forward(self, pred_theta: torch.Tensor, pred_beta: torch.Tensor,\n",
    "                gt_theta: torch.Tensor, gt_beta: torch.Tensor,\n",
    "                gt_joints: Optional[torch.Tensor] = None,\n",
    "                gt_vertices: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:\n",
    "        losses = {}\n",
    "        \n",
    "        # 1. Parameter loss\n",
    "        loss_param_theta = self.mse_loss(pred_theta, gt_theta)\n",
    "        loss_param_beta = self.mse_loss(pred_beta, gt_beta)\n",
    "        loss_param = loss_param_theta + 0.1 * loss_param_beta\n",
    "        losses['param'] = loss_param\n",
    "        \n",
    "        # 2. Joint position loss (if MANO layer available)\n",
    "        if gt_joints is not None and self.mano_layer is not None:\n",
    "            pred_verts, pred_joints = self.mano_layer(pred_theta, pred_beta)\n",
    "            loss_joint = self.mse_loss(pred_joints, gt_joints)\n",
    "            losses['joint'] = loss_joint\n",
    "        else:\n",
    "            loss_joint = torch.tensor(0.0, device=pred_theta.device)\n",
    "            losses['joint'] = loss_joint\n",
    "        \n",
    "        # 3. Vertex loss (optional)\n",
    "        if self.use_vertex_loss and gt_vertices is not None and self.mano_layer is not None:\n",
    "            if 'pred_verts' not in locals():\n",
    "                pred_verts, _ = self.mano_layer(pred_theta, pred_beta)\n",
    "            loss_vertex = self.mse_loss(pred_verts, gt_vertices)\n",
    "            losses['vertex'] = loss_vertex\n",
    "        else:\n",
    "            loss_vertex = torch.tensor(0.0, device=pred_theta.device)\n",
    "            losses['vertex'] = loss_vertex\n",
    "        \n",
    "        # 4. Regularization\n",
    "        loss_reg_theta = (pred_theta ** 2).mean()\n",
    "        loss_reg_beta = (pred_beta ** 2).mean()\n",
    "        loss_reg = loss_reg_theta + loss_reg_beta\n",
    "        losses['reg'] = loss_reg\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = (\n",
    "            self.lambda_param * loss_param +\n",
    "            self.lambda_joint * loss_joint +\n",
    "            self.lambda_vertex * loss_vertex +\n",
    "            self.lambda_reg * loss_reg\n",
    "        )\n",
    "        losses['total'] = total_loss\n",
    "        \n",
    "        return losses\n",
    "\n",
    "print(\"âœ“ Loss functions defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Define Dataset Loader (FreiHAND)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATASET LOADER (FreiHAND)\n",
    "# ============================================================================\n",
    "\n",
    "class FreiHANDDataset(Dataset):\n",
    "    \"\"\"FreiHAND dataset loader - 32K poses Ã— 4 views = 130K images.\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_path: str, split: str = \"train\", use_augmentation: bool = True,\n",
    "                 image_size: int = 224, use_all_views: bool = True):\n",
    "        self.root = Path(dataset_path)\n",
    "        self.split = split\n",
    "        self.image_size = image_size\n",
    "        self.use_all_views = use_all_views\n",
    "        \n",
    "        # Load annotations\n",
    "        mano_path = self.root / \"training_mano.json\"\n",
    "        xyz_path = self.root / \"training_xyz.json\"\n",
    "        \n",
    "        if not mano_path.exists():\n",
    "            raise FileNotFoundError(f\"MANO annotations not found: {mano_path}\")\n",
    "        \n",
    "        with open(mano_path, 'r') as f:\n",
    "            mano_data = json.load(f)\n",
    "        with open(xyz_path, 'r') as f:\n",
    "            xyz_data = json.load(f)\n",
    "        \n",
    "        # Extract MANO parameters\n",
    "        self.mano_params = []\n",
    "        self.joints_3d = np.array(xyz_data)  # [32560, 21, 3]\n",
    "        \n",
    "        for i, mano_dict in enumerate(mano_data):\n",
    "            params = {\n",
    "                'theta': np.array(mano_dict['hand_pose'], dtype=np.float32),  # [45]\n",
    "                'beta': np.array(mano_dict['betas'], dtype=np.float32),  # [10]\n",
    "                'trans': np.array(mano_dict.get('hand_trans', [0, 0, 0]), dtype=np.float32),\n",
    "            }\n",
    "            self.mano_params.append(params)\n",
    "        \n",
    "        # Image paths\n",
    "        self.rgb_dir = self.root / \"training\" / \"rgb\"\n",
    "        \n",
    "        # Create sample indices\n",
    "        self.samples = []\n",
    "        num_unique = len(self.mano_params)  # 32560\n",
    "        \n",
    "        if use_all_views:\n",
    "            # Use all 4 views per pose (130K samples)\n",
    "            for base_idx in range(num_unique):\n",
    "                for view_idx in range(4):\n",
    "                    img_idx = base_idx * 4 + view_idx\n",
    "                    self.samples.append({\n",
    "                        'base_idx': base_idx,\n",
    "                        'view_idx': view_idx,\n",
    "                        'img_idx': img_idx,\n",
    "                    })\n",
    "        else:\n",
    "            # Use only first view (32K samples)\n",
    "            for base_idx in range(num_unique):\n",
    "                self.samples.append({\n",
    "                    'base_idx': base_idx,\n",
    "                    'view_idx': 0,\n",
    "                    'img_idx': base_idx * 4,\n",
    "                })\n",
    "        \n",
    "        print(f\"[FreiHAND] Loaded {len(self.samples)} samples\")\n",
    "        print(f\"  Unique poses: {num_unique}\")\n",
    "        print(f\"  Views per pose: {4 if use_all_views else 1}\")\n",
    "        \n",
    "        # Data augmentation\n",
    "        if use_augmentation and split == \"train\":\n",
    "            self.transform = A.Compose([\n",
    "                A.Resize(image_size, image_size),\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.Rotate(limit=15, p=0.5),\n",
    "                A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n",
    "                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                ToTensorV2(),\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = A.Compose([\n",
    "                A.Resize(image_size, image_size),\n",
    "                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                ToTensorV2(),\n",
    "            ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        base_idx = sample['base_idx']\n",
    "        img_idx = sample['img_idx']\n",
    "        \n",
    "        # Load image\n",
    "        img_path = self.rgb_dir / f\"{img_idx:08d}.jpg\"\n",
    "        image = cv2.imread(str(img_path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Apply transforms\n",
    "        transformed = self.transform(image=image)\n",
    "        image_tensor = transformed['image']  # [3, H, W]\n",
    "        \n",
    "        # Get MANO parameters\n",
    "        mano = self.mano_params[base_idx]\n",
    "        theta = torch.from_numpy(mano['theta'])  # [45]\n",
    "        beta = torch.from_numpy(mano['beta'])  # [10]\n",
    "        \n",
    "        # Get 3D joints\n",
    "        joints_3d = torch.from_numpy(self.joints_3d[base_idx])  # [21, 3]\n",
    "        \n",
    "        return {\n",
    "            'image': image_tensor,\n",
    "            'theta': theta,\n",
    "            'beta': beta,\n",
    "            'joints_3d': joints_3d,\n",
    "            'base_idx': base_idx,\n",
    "            'view_idx': sample['view_idx'],\n",
    "        }\n",
    "\n",
    "\n",
    "def create_dataloaders(freihand_path: str, ho3d_path: Optional[str] = None,\n",
    "                       batch_size: int = 32, num_workers: int = 4, image_size: int = 224,\n",
    "                       use_all_views: bool = True, train_split: float = 0.9):\n",
    "    \"\"\"\n",
    "    Create train and validation dataloaders.\n",
    "    \n",
    "    Args:\n",
    "        freihand_path: path to FreiHAND dataset\n",
    "        ho3d_path: optional path to HO-3D dataset (not implemented yet)\n",
    "        batch_size: batch size for training\n",
    "        num_workers: number of data loading workers\n",
    "        image_size: target image size\n",
    "        use_all_views: use all 4 views per FreiHAND pose (130K samples) or False (32K samples)\n",
    "        train_split: fraction of data for training (default 0.9 = 90% train, 10% val)\n",
    "    \n",
    "    Returns:\n",
    "        train_loader, val_loader\n",
    "    \"\"\"\n",
    "    # Create FreiHAND dataset\n",
    "    freihand_full = FreiHANDDataset(\n",
    "        dataset_path=freihand_path,\n",
    "        split=\"train\",\n",
    "        use_augmentation=True,\n",
    "        image_size=image_size,\n",
    "        use_all_views=use_all_views,\n",
    "    )\n",
    "    \n",
    "    # Split into train/val (90% train, 10% val by default)\n",
    "    total_size = len(freihand_full)\n",
    "    train_size = int(train_split * total_size)\n",
    "    val_size = total_size - train_size\n",
    "    \n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        freihand_full,\n",
    "        [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(42)  # Reproducible split\n",
    "    )\n",
    "    \n",
    "    # HO-3D integration (placeholder - not implemented yet)\n",
    "    if ho3d_path:\n",
    "        print(\"âš ï¸  HO-3D dataset support not yet implemented\")\n",
    "        print(\"   Set ho3d_path=None to use only FreiHAND\")\n",
    "        # TODO: Implement HO3DDataset and combine\n",
    "    \n",
    "    # Create dataloaders with GPU optimizations\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        prefetch_factor=2 if torch.cuda.is_available() else None,\n",
    "        persistent_workers=True if num_workers > 0 and torch.cuda.is_available() else False,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        prefetch_factor=2 if torch.cuda.is_available() else None,\n",
    "        persistent_workers=True if num_workers > 0 and torch.cuda.is_available() else False,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n[DataLoaders]\")\n",
    "    print(f\"  Train: {len(train_dataset)} samples ({train_split*100:.0f}%)\")\n",
    "    print(f\"  Val: {len(val_dataset)} samples ({(1-train_split)*100:.0f}%)\")\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "print(\"âœ“ Dataset loader defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MANO LAYER (for forward pass: parameters â†’ 3D mesh)\n",
    "# ============================================================================\n",
    "\n",
    "class CustomMANOLayer:\n",
    "    \"\"\"Custom MANO model loader - converts parameters to 3D hand mesh.\"\"\"\n",
    "    \n",
    "    def __init__(self, mano_root: str, side: str = \"right\", device: str = \"cpu\"):\n",
    "        self.mano_root = Path(mano_root)\n",
    "        self.side = side.upper()\n",
    "        self.device = device\n",
    "        self.data = self._load_mano_model()\n",
    "        self._setup_tensors()\n",
    "    \n",
    "    def _load_mano_model(self):\n",
    "        \"\"\"Load MANO model from converted numpy pkl file.\"\"\"\n",
    "        pkl_path = self.mano_root / f\"MANO_{self.side}_numpy.pkl\"\n",
    "        if not pkl_path.exists():\n",
    "            raise FileNotFoundError(f\"MANO model not found: {pkl_path}\")\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    \n",
    "    def _setup_tensors(self):\n",
    "        \"\"\"Convert numpy arrays to PyTorch tensors.\"\"\"\n",
    "        self.shapedirs = torch.from_numpy(self.data['shapedirs'].astype(np.float32)).to(self.device)\n",
    "        self.posedirs = torch.from_numpy(self.data['posedirs'].astype(np.float32)).to(self.device)\n",
    "        self.v_template = torch.from_numpy(self.data['v_template'].astype(np.float32)).to(self.device)\n",
    "        self.J_regressor = torch.from_numpy(self.data['J_regressor'].toarray().astype(np.float32)).to(self.device)\n",
    "        self.weights = torch.from_numpy(self.data['weights'].astype(np.float32)).to(self.device)\n",
    "        self.kintree_table = self.data['kintree_table']\n",
    "        self.faces = torch.from_numpy(self.data['f'].astype(np.int64)).to(self.device)\n",
    "        self.fingertip_indices = torch.tensor([745, 317, 444, 556, 673], dtype=torch.long, device=self.device)\n",
    "    \n",
    "    def rodrigues(self, theta: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Convert axis-angle to rotation matrix using Rodrigues formula.\"\"\"\n",
    "        batch_size = theta.shape[0]\n",
    "        device = theta.device\n",
    "        angle = torch.norm(theta + 1e-8, dim=1, keepdim=True)\n",
    "        r = theta / angle\n",
    "        rx, ry, rz = r[:, 0], r[:, 1], r[:, 2]\n",
    "        K = torch.zeros((batch_size, 3, 3), device=device)\n",
    "        K[:, 0, 1] = -rz; K[:, 0, 2] = ry\n",
    "        K[:, 1, 0] = rz; K[:, 1, 2] = -rx\n",
    "        K[:, 2, 0] = -ry; K[:, 2, 1] = rx\n",
    "        K_squared = torch.bmm(K, K)\n",
    "        angle = angle.unsqueeze(-1)\n",
    "        I = torch.eye(3, device=device).unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "        R = I + torch.sin(angle) * K + (1 - torch.cos(angle)) * K_squared\n",
    "        return R\n",
    "    \n",
    "    def batch_rodrigues(self, theta: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Apply Rodrigues to multiple axis-angle vectors.\"\"\"\n",
    "        batch_size = theta.shape[0]\n",
    "        if theta.dim() == 2:\n",
    "            theta = theta.view(batch_size, -1, 3)\n",
    "        num_joints = theta.shape[1]\n",
    "        theta_flat = theta.view(-1, 3)\n",
    "        R_flat = self.rodrigues(theta_flat)\n",
    "        return R_flat.view(batch_size, num_joints, 3, 3)\n",
    "    \n",
    "    def with_zeros(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Append homogeneous row [0,0,0,1] to 3x4 matrix.\"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        bottom = torch.tensor([0.0, 0.0, 0.0, 1.0], device=x.device).unsqueeze(0).unsqueeze(0)\n",
    "        bottom = bottom.repeat(batch_size, 1, 1)\n",
    "        return torch.cat([x, bottom], dim=1)\n",
    "    \n",
    "    def pack(self, R: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Pack rotation and translation into 4x4 homogeneous matrix.\"\"\"\n",
    "        transform_3x4 = torch.cat([R, t], dim=2)\n",
    "        return self.with_zeros(transform_3x4)\n",
    "    \n",
    "    def forward(self, pose: torch.Tensor, betas: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Forward pass: parameters â†’ 3D hand mesh.\"\"\"\n",
    "        batch_size = pose.shape[0]\n",
    "        device = pose.device\n",
    "        \n",
    "        # Step 1: Shape blend shapes\n",
    "        v_shaped = self.v_template + torch.einsum('vij,bj->bvi', self.shapedirs, betas)\n",
    "        \n",
    "        # Step 2: Rest pose joints\n",
    "        J = torch.einsum('jv,bvi->bji', self.J_regressor, v_shaped)  # [B, 16, 3]\n",
    "        \n",
    "        # Step 3: Pose rotations\n",
    "        pose_cube = pose.view(batch_size, -1, 3)  # [B, 15, 3]\n",
    "        rot_mats = self.batch_rodrigues(pose_cube)  # [B, 15, 3, 3]\n",
    "        \n",
    "        # Step 4: Pose blend shapes\n",
    "        I_cube = torch.eye(3, device=device).unsqueeze(0).unsqueeze(0).repeat(batch_size, rot_mats.shape[1], 1, 1)\n",
    "        pose_feature = (rot_mats - I_cube).view(batch_size, -1)  # [B, 135]\n",
    "        \n",
    "        if self.posedirs.dim() == 3:\n",
    "            v_posed = v_shaped + torch.einsum('vij,bj->bvi', self.posedirs, pose_feature)\n",
    "        else:\n",
    "            v_posed = v_shaped + torch.einsum('ij,bj->bi', self.posedirs, pose_feature).view(batch_size, -1, 3)\n",
    "        \n",
    "        # Step 5: Forward kinematics\n",
    "        num_joints = J.shape[1]\n",
    "        G_list = []\n",
    "        for i in range(num_joints):\n",
    "            parent_idx = int(self.kintree_table[0, i]) if i > 0 else -1\n",
    "            if i == 0 or parent_idx < 0:\n",
    "                R_i = rot_mats[:, i, :, :] if i < rot_mats.shape[1] else torch.eye(3, device=device).unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "                t_i = J[:, i, :].unsqueeze(-1)\n",
    "                G_i = self.pack(R_i, t_i)\n",
    "                G_list.append(G_i)\n",
    "            else:\n",
    "                R_i = rot_mats[:, i, :, :] if i < rot_mats.shape[1] else torch.eye(3, device=device).unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "                t_rel = (J[:, i, :] - J[:, parent_idx, :]).unsqueeze(-1)\n",
    "                T_local = self.pack(R_i, t_rel)\n",
    "                G_i = torch.matmul(G_list[parent_idx], T_local)\n",
    "                G_list.append(G_i)\n",
    "        \n",
    "        G = torch.stack(G_list, dim=1)  # [B, 16, 4, 4]\n",
    "        \n",
    "        # Skinning transforms\n",
    "        G_skinning_list = []\n",
    "        for i in range(num_joints):\n",
    "            J_rest_inv = -J[:, i, :].unsqueeze(-1)\n",
    "            I_batch = torch.eye(3, device=device).unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "            T_rest_inv = self.pack(I_batch, J_rest_inv)\n",
    "            G_skin_i = torch.matmul(G[:, i, :, :].clone(), T_rest_inv)\n",
    "            G_skinning_list.append(G_skin_i)\n",
    "        G_skinning = torch.stack(G_skinning_list, dim=1)  # [B, 16, 4, 4]\n",
    "        \n",
    "        # Step 6: Linear blend skinning\n",
    "        v_posed_homo = torch.cat([v_posed, torch.ones((batch_size, v_posed.shape[1], 1), device=device)], dim=2)\n",
    "        W = self.weights.unsqueeze(0).repeat(batch_size, 1, 1)  # [B, 778, 16]\n",
    "        T = torch.einsum('bjkl,bvl->bvjk', G_skinning, v_posed_homo)  # [B, 778, 16, 4]\n",
    "        v_final_homo = torch.einsum('bvj,bvjk->bvk', W, T)  # [B, 778, 4]\n",
    "        vertices = v_final_homo[:, :, :3]\n",
    "        \n",
    "        # Joint positions\n",
    "        joints_posed = G[:, :, :3, 3]  # [B, 16, 3]\n",
    "        J_fingertips = vertices[:, self.fingertip_indices, :]  # [B, 5, 3]\n",
    "        joints_final = torch.cat([joints_posed, J_fingertips], dim=1)  # [B, 21, 3]\n",
    "        \n",
    "        return vertices, joints_final\n",
    "    \n",
    "    def __call__(self, pose: torch.Tensor, betas: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.forward(pose, betas)\n",
    "\n",
    "print(\"âœ“ MANO layer defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders\n",
    "print(f\"\\n[Creating DataLoaders]\")\n",
    "print(f\"  dataset: {config['dataset_path']}\")\n",
    "print(f\"  batch size: {config['batch_size']}\")\n",
    "print(f\"  num workers: {config['num_workers']}\")\n",
    "\n",
    "train_loader, val_loader = create_dataloaders(\n",
    "    freihand_path=config['dataset_path'],\n",
    "    ho3d_path=None,  # not using ho3d for now\n",
    "    batch_size=config['batch_size'],\n",
    "    num_workers=config['num_workers'],\n",
    "    image_size=config['image_size'],\n",
    "    use_all_views=config['use_all_views'],\n",
    "    train_split=config['train_split'],\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ dataloaders created:\")\n",
    "print(f\"  train batches: {len(train_loader)}\")\n",
    "print(f\"  val batches: {len(val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device\n",
    "device = torch.device(f\"cuda:{config['gpu']}\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create model\n",
    "print(f\"\\n[Creating model: {config['architecture']}]\")\n",
    "model = create_model(\n",
    "    architecture=config['architecture'],\n",
    "    backbone=config['backbone'],\n",
    "    pretrained=config['pretrained'],\n",
    "    predict_beta=config['predict_beta'],\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"  Total parameters: {num_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Setup Loss, Optimizer, and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MANO layer (for MANO-aware loss)\n",
    "mano_layer = None\n",
    "if config['use_mano_loss']:\n",
    "    print(\"[Loading MANO layer for loss computation]\")\n",
    "    mano_root = MANO_ROOT if 'MANO_ROOT' in globals() else Path(REPO_ROOT / 'models')\n",
    "    mano_layer = CustomMANOLayer(\n",
    "        mano_root=str(mano_root),\n",
    "        side=\"right\",\n",
    "        device=str(device)\n",
    "    )\n",
    "    print(\"âœ“ MANO layer loaded\")\n",
    "else:\n",
    "    print(\"Using simple parameter loss (faster)\")\n",
    "\n",
    "# Loss function\n",
    "if config['use_mano_loss']:\n",
    "    criterion = MANOLoss(\n",
    "        lambda_param=config['lambda_param'],\n",
    "        lambda_joint=config['lambda_joint'],\n",
    "        lambda_reg=config['lambda_reg'],\n",
    "        mano_layer=mano_layer,\n",
    "    )\n",
    "else:\n",
    "    criterion = SimpleMANOLoss(\n",
    "        lambda_param=config['lambda_param'],\n",
    "        lambda_reg=config['lambda_reg'],\n",
    "    )\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config['lr'],\n",
    "    weight_decay=config['weight_decay'],\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "if config['lr_schedule'] == \"cosine\":\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=config['epochs']\n",
    "    )\n",
    "elif config['lr_schedule'] == \"step\":\n",
    "    scheduler = optim.lr_scheduler.StepLR(\n",
    "        optimizer, step_size=config['epochs'] // 3, gamma=0.1\n",
    "    )\n",
    "else:\n",
    "    scheduler = None\n",
    "\n",
    "# Mixed precision scaler (for FP16 training on A100)\n",
    "scaler = None\n",
    "if config['use_mixed_precision'] and torch.cuda.is_available():\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    print(\"âœ“ Mixed precision (FP16) enabled for faster training\")\n",
    "\n",
    "print(\"âœ“ Loss, optimizer, and scheduler configured\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Setup Logging and Checkpointing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory structure (save to both Drive and local)\n",
    "import shutil\n",
    "\n",
    "output_dir_drive = Path(config['output_dir_drive'])\n",
    "output_dir_local = Path(config['output_dir_local'])\n",
    "output_dir_drive.mkdir(parents=True, exist_ok=True)\n",
    "output_dir_local.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create experiment name with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "experiment_name = f\"{config['architecture']}_{config['backbone']}_{timestamp}\"\n",
    "\n",
    "# Create experiment directories\n",
    "experiment_dir_drive = output_dir_drive / experiment_name\n",
    "experiment_dir_local = output_dir_local / experiment_name\n",
    "experiment_dir_drive.mkdir(parents=True, exist_ok=True)\n",
    "experiment_dir_local.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Use Drive as primary\n",
    "experiment_dir = experiment_dir_drive\n",
    "\n",
    "# TensorBoard (save to Drive)\n",
    "log_dir = experiment_dir / \"logs\"\n",
    "writer = SummaryWriter(log_dir=str(log_dir))\n",
    "print(f\"âœ“ TensorBoard logs: {log_dir}\")\n",
    "\n",
    "# Save config (to both locations)\n",
    "config_path_drive = experiment_dir_drive / \"config.json\"\n",
    "config_path_local = experiment_dir_local / \"config.json\"\n",
    "with open(config_path_drive, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "shutil.copy(config_path_drive, config_path_local)\n",
    "print(f\"âœ“ Config saved to Drive and local\")\n",
    "\n",
    "print(f\"âœ“ Checkpoints will be saved to:\")\n",
    "print(f\"  - Primary (Google Drive): {experiment_dir_drive}\")\n",
    "print(f\"  - Backup (Local): {experiment_dir_local}\")\n",
    "\n",
    "# Estimate storage requirements\n",
    "print(f\"\\nStorage estimate:\")\n",
    "print(f\"  - best.pth: ~300MB (ResNet) / ~500MB (Transformer)\")\n",
    "print(f\"  - latest.pth: ~300MB (ResNet) / ~500MB (Transformer)\")\n",
    "if config['save_periodic_checkpoints']:\n",
    "    num_periodic = config['epochs'] // 10\n",
    "    print(f\"  - Periodic checkpoints ({num_periodic} files): ~{num_periodic * 300}MB (ResNet) / ~{num_periodic * 500}MB (Transformer)\")\n",
    "    print(f\"  - Total per experiment: ~{2 + num_periodic}GB (ResNet) / ~{3 + num_periodic * 0.5}GB (Transformer)\")\n",
    "else:\n",
    "    print(f\"  - Periodic checkpoints: Disabled (saves ~1-2GB)\")\n",
    "    print(f\"  - Total per experiment: ~600MB-1GB\")\n",
    "print(f\"\\nNote: Checkpoints are gitignored. Clean up old experiments manually if needed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Training Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device, epoch, use_mano_loss=False, mano_layer=None, scaler=None, use_mixed_precision=False):\n",
    "    \"\"\"Train for one epoch with optional mixed precision.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    loss_components = {}\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        images = batch['image'].to(device, non_blocking=True)\n",
    "        gt_theta = batch['theta'].to(device, non_blocking=True)\n",
    "        gt_beta = batch['beta'].to(device, non_blocking=True)\n",
    "        \n",
    "        # Mixed precision forward pass\n",
    "        if use_mixed_precision and scaler is not None:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                # Forward pass\n",
    "                if hasattr(model, 'predict_beta') and model.predict_beta:\n",
    "                    pred_theta, pred_beta = model(images)\n",
    "                else:\n",
    "                    pred_theta = model(images)\n",
    "                    pred_beta = torch.zeros_like(gt_beta)\n",
    "                \n",
    "                # Compute loss\n",
    "                if use_mano_loss and mano_layer is not None:\n",
    "                    gt_joints = batch['joints_3d'].to(device, non_blocking=True)\n",
    "                    losses = criterion(pred_theta, pred_beta, gt_theta, gt_beta, gt_joints=gt_joints)\n",
    "                else:\n",
    "                    losses = criterion(pred_theta, pred_beta, gt_theta, gt_beta)\n",
    "                \n",
    "                loss = losses['total']\n",
    "            \n",
    "            # Backward pass with mixed precision\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            # Standard precision forward pass\n",
    "            if hasattr(model, 'predict_beta') and model.predict_beta:\n",
    "                pred_theta, pred_beta = model(images)\n",
    "            else:\n",
    "                pred_theta = model(images)\n",
    "                pred_beta = torch.zeros_like(gt_beta)\n",
    "            \n",
    "            # Compute loss\n",
    "            if use_mano_loss and mano_layer is not None:\n",
    "                gt_joints = batch['joints_3d'].to(device, non_blocking=True)\n",
    "                losses = criterion(pred_theta, pred_beta, gt_theta, gt_beta, gt_joints=gt_joints)\n",
    "            else:\n",
    "                losses = criterion(pred_theta, pred_beta, gt_theta, gt_beta)\n",
    "            \n",
    "            loss = losses['total']\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Accumulate losses\n",
    "        total_loss += loss.item()\n",
    "        for key, value in losses.items():\n",
    "            if key not in loss_components:\n",
    "                loss_components[key] = 0.0\n",
    "            loss_components[key] += value.item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    # Average losses\n",
    "    num_batches = len(dataloader)\n",
    "    avg_loss = total_loss / num_batches\n",
    "    for key in loss_components:\n",
    "        loss_components[key] /= num_batches\n",
    "    loss_components['total'] = avg_loss\n",
    "    \n",
    "    return loss_components\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, device, use_mano_loss=False, mano_layer=None):\n",
    "    \"\"\"Validate model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    loss_components = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Validation\"):\n",
    "            images = batch['image'].to(device)\n",
    "            gt_theta = batch['theta'].to(device)\n",
    "            gt_beta = batch['beta'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            if hasattr(model, 'predict_beta') and model.predict_beta:\n",
    "                pred_theta, pred_beta = model(images)\n",
    "            else:\n",
    "                pred_theta = model(images)\n",
    "                pred_beta = torch.zeros_like(gt_beta)\n",
    "            \n",
    "            # Compute loss\n",
    "            if use_mano_loss and mano_layer is not None:\n",
    "                gt_joints = batch['joints_3d'].to(device)\n",
    "                losses = criterion(pred_theta, pred_beta, gt_theta, gt_beta, gt_joints=gt_joints)\n",
    "            else:\n",
    "                losses = criterion(pred_theta, pred_beta, gt_theta, gt_beta)\n",
    "            \n",
    "            loss = losses['total']\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            for key, value in losses.items():\n",
    "                if key not in loss_components:\n",
    "                    loss_components[key] = 0.0\n",
    "                loss_components[key] += value.item()\n",
    "    \n",
    "    # Average losses\n",
    "    num_batches = len(dataloader)\n",
    "    avg_loss = total_loss / num_batches\n",
    "    for key in loss_components:\n",
    "        loss_components[key] /= num_batches\n",
    "    loss_components['total'] = avg_loss\n",
    "    \n",
    "    return loss_components\n",
    "\n",
    "print(\"âœ“ Training functions defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'lr': [],\n",
    "}\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Starting training for {config['epochs']} epochs\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "for epoch in range(config['epochs']):\n",
    "    # Train\n",
    "    train_losses = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, device, epoch,\n",
    "        config['use_mano_loss'], mano_layer, scaler, config['use_mixed_precision']\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_losses = validate(\n",
    "        model, val_loader, criterion, device,\n",
    "        config['use_mano_loss'], mano_layer\n",
    "    )\n",
    "    \n",
    "    # Update learning rate\n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "    \n",
    "    # Log to TensorBoard\n",
    "    for key, value in train_losses.items():\n",
    "        writer.add_scalar(f'Train/{key}', value, epoch)\n",
    "    for key, value in val_losses.items():\n",
    "        writer.add_scalar(f'Val/{key}', value, epoch)\n",
    "    writer.add_scalar('LR', optimizer.param_groups[0]['lr'], epoch)\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_losses['total'])\n",
    "    history['val_loss'].append(val_losses['total'])\n",
    "    history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"\\nEpoch {epoch+1}/{config['epochs']}\")\n",
    "    print(f\"  Train loss: {train_losses['total']:.4f}\")\n",
    "    print(f\"  Val loss: {val_losses['total']:.4f}\")\n",
    "    print(f\"  LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    is_best = val_losses['total'] < best_val_loss\n",
    "    if is_best:\n",
    "        best_val_loss = val_losses['total']\n",
    "    \n",
    "    checkpoint = {\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "        'train_loss': train_losses['total'],\n",
    "        'val_loss': val_losses['total'],\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'config': config,\n",
    "        'history': history,\n",
    "    }\n",
    "    \n",
    "    # Save latest (to both Drive and local)\n",
    "    torch.save(checkpoint, experiment_dir_drive / \"latest.pth\")\n",
    "    torch.save(checkpoint, experiment_dir_local / \"latest.pth\")\n",
    "    \n",
    "    # Save best (to both Drive and local)\n",
    "    if is_best:\n",
    "        torch.save(checkpoint, experiment_dir_drive / \"best.pth\")\n",
    "        torch.save(checkpoint, experiment_dir_local / \"best.pth\")\n",
    "        print(f\"  âœ“ Saved best model (val_loss: {best_val_loss:.4f})\")\n",
    "    \n",
    "    # Save periodic checkpoints (optional, adds ~1-2GB per experiment)\n",
    "    if config['save_periodic_checkpoints'] and (epoch + 1) % 10 == 0:\n",
    "        checkpoint_path_drive = experiment_dir_drive / f\"checkpoint_epoch_{epoch+1}.pth\"\n",
    "        checkpoint_path_local = experiment_dir_local / f\"checkpoint_epoch_{epoch+1}.pth\"\n",
    "        torch.save(checkpoint, checkpoint_path_drive)\n",
    "        torch.save(checkpoint, checkpoint_path_local)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training complete!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"Checkpoints saved to: {experiment_dir}\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Training History Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_loss'], label='Train')\n",
    "plt.plot(history['val_loss'], label='Val')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training History')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['lr'], label='Learning Rate')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('LR')\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(experiment_dir / \"training_history.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ“ Training history saved: {experiment_dir / 'training_history.png'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Evaluation\n",
    "\n",
    "Evaluate the best model on validation set with detailed metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "print(\"[Loading best checkpoint]\")\n",
    "checkpoint = torch.load(experiment_dir / \"best.pth\", map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"âœ“ Loaded epoch {checkpoint['epoch']} (val_loss: {checkpoint['val_loss']:.4f})\")\n",
    "\n",
    "# Evaluation function\n",
    "def compute_mpjpe(pred_joints, gt_joints):\n",
    "    \"\"\"Compute Mean Per-Joint Position Error (MPJPE) in mm.\"\"\"\n",
    "    error = torch.norm(pred_joints - gt_joints, dim=2)  # [B, 21]\n",
    "    mpjpe = error.mean() * 1000  # convert to mm\n",
    "    return mpjpe.item()\n",
    "\n",
    "# Evaluate\n",
    "model.eval()\n",
    "param_errors_theta = []\n",
    "param_errors_beta = []\n",
    "joint_errors = []\n",
    "per_joint_errors = []\n",
    "\n",
    "# Load MANO layer for evaluation\n",
    "mano_root = MANO_ROOT if 'MANO_ROOT' in globals() else Path(REPO_ROOT / 'models')\n",
    "mano_layer_eval = CustomMANOLayer(\n",
    "    mano_root=str(mano_root),\n",
    "    side=\"right\",\n",
    "    device=str(device)\n",
    ")\n",
    "\n",
    "print(\"\\n[Evaluating on validation set]\")\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader):\n",
    "        images = batch['image'].to(device)\n",
    "        gt_theta = batch['theta'].to(device)\n",
    "        gt_beta = batch['beta'].to(device)\n",
    "        gt_joints = batch['joints_3d'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        if hasattr(model, 'predict_beta') and model.predict_beta:\n",
    "            pred_theta, pred_beta = model(images)\n",
    "        else:\n",
    "            pred_theta = model(images)\n",
    "            pred_beta = torch.zeros_like(gt_beta)\n",
    "        \n",
    "        # Parameter errors\n",
    "        theta_error = torch.norm(pred_theta - gt_theta, dim=1).mean().item()\n",
    "        beta_error = torch.norm(pred_beta - gt_beta, dim=1).mean().item()\n",
    "        param_errors_theta.append(theta_error)\n",
    "        param_errors_beta.append(beta_error)\n",
    "        \n",
    "        # Forward through MANO to get joints\n",
    "        pred_verts, pred_joints = mano_layer_eval(pred_theta, pred_beta)\n",
    "        \n",
    "        # Joint position errors\n",
    "        batch_mpjpe = compute_mpjpe(pred_joints, gt_joints)\n",
    "        joint_errors.append(batch_mpjpe)\n",
    "        \n",
    "        # Per-joint errors\n",
    "        per_joint_error = torch.norm(pred_joints - gt_joints, dim=2) * 1000  # [B, 21] in mm\n",
    "        per_joint_errors.append(per_joint_error.cpu().numpy())\n",
    "\n",
    "# Aggregate results\n",
    "results = {\n",
    "    'param_error_theta': {\n",
    "        'mean': np.mean(param_errors_theta),\n",
    "        'std': np.std(param_errors_theta),\n",
    "        'median': np.median(param_errors_theta),\n",
    "    },\n",
    "    'param_error_beta': {\n",
    "        'mean': np.mean(param_errors_beta),\n",
    "        'std': np.std(param_errors_beta),\n",
    "        'median': np.median(param_errors_beta),\n",
    "    },\n",
    "    'joint_error_mm': {\n",
    "        'mean': np.mean(joint_errors),\n",
    "        'std': np.std(joint_errors),\n",
    "        'median': np.median(joint_errors),\n",
    "        'p95': np.percentile(joint_errors, 95),\n",
    "    },\n",
    "    'per_joint_error_mm': np.concatenate(per_joint_errors, axis=0).mean(axis=0).tolist(),\n",
    "}\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nParameter Errors:\")\n",
    "print(f\"  Î¸ (pose): {results['param_error_theta']['mean']:.4f} Â± {results['param_error_theta']['std']:.4f}\")\n",
    "print(f\"  Î² (shape): {results['param_error_beta']['mean']:.4f} Â± {results['param_error_beta']['std']:.4f}\")\n",
    "print(f\"\\nJoint Position Error (MPJPE):\")\n",
    "print(f\"  Mean: {results['joint_error_mm']['mean']:.2f} mm\")\n",
    "print(f\"  Median: {results['joint_error_mm']['median']:.2f} mm\")\n",
    "print(f\"  95th percentile: {results['joint_error_mm']['p95']:.2f} mm\")\n",
    "\n",
    "# Save results\n",
    "# Save results (to both Drive and local)\n",
    "results_path_drive = experiment_dir_drive / \"evaluation_results.json\"\n",
    "results_path_local = experiment_dir_local / \"evaluation_results.json\"\n",
    "\n",
    "with open(results_path_drive, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "shutil.copy(results_path_drive, results_path_local)\n",
    "\n",
    "print(f\"\\nâœ“ Results saved to:\")\n",
    "print(f\"  - Google Drive: {results_path_drive}\")\n",
    "print(f\"  - Local backup: {results_path_local}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Evaluation Results Visualization\n",
    "\n",
    "Plot evaluation metrics: per-joint errors, error distributions, and comparisons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot evaluation results\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# joint names for mano hand (21 joints)\n",
    "joint_names = [\n",
    "    'Wrist', 'Thumb1', 'Thumb2', 'Thumb3', 'Thumb4',\n",
    "    'Index1', 'Index2', 'Index3', 'Index4',\n",
    "    'Middle1', 'Middle2', 'Middle3', 'Middle4',\n",
    "    'Ring1', 'Ring2', 'Ring3', 'Ring4',\n",
    "    'Pinky1', 'Pinky2', 'Pinky3', 'Pinky4',\n",
    "]\n",
    "\n",
    "# create figure with subplots\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "\n",
    "# 1. per-joint error bar chart\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "per_joint_means = results['per_joint_error_mm']\n",
    "x_pos = np.arange(len(joint_names))\n",
    "ax1.bar(x_pos, per_joint_means, alpha=0.7, color='steelblue')\n",
    "ax1.set_xlabel('Joint')\n",
    "ax1.set_ylabel('Error (mm)')\n",
    "ax1.set_title('Per-Joint Position Error')\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(joint_names, rotation=45, ha='right', fontsize=8)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.axhline(y=results['joint_error_mm']['mean'], color='r', linestyle='--', label=f\"Mean MPJPE: {results['joint_error_mm']['mean']:.2f}mm\")\n",
    "ax1.legend()\n",
    "\n",
    "# 2. joint error distribution histogram\n",
    "ax2 = plt.subplot(2, 3, 2)\n",
    "ax2.hist(joint_errors, bins=50, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "ax2.axvline(results['joint_error_mm']['mean'], color='r', linestyle='--', label=f\"Mean: {results['joint_error_mm']['mean']:.2f}mm\")\n",
    "ax2.axvline(results['joint_error_mm']['median'], color='g', linestyle='--', label=f\"Median: {results['joint_error_mm']['median']:.2f}mm\")\n",
    "ax2.set_xlabel('MPJPE (mm)')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Joint Error Distribution')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. parameter error comparison\n",
    "ax3 = plt.subplot(2, 3, 3)\n",
    "param_errors = [\n",
    "    results['param_error_theta']['mean'],\n",
    "    results['param_error_beta']['mean'],\n",
    "]\n",
    "param_labels = ['Î¸ (pose)', 'Î² (shape)']\n",
    "colors = ['steelblue', 'coral']\n",
    "bars = ax3.bar(param_labels, param_errors, alpha=0.7, color=colors)\n",
    "ax3.set_ylabel('Error')\n",
    "ax3.set_title('Parameter Errors')\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "# add value labels on bars\n",
    "for bar, err in zip(bars, param_errors):\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{err:.4f}', ha='center', va='bottom')\n",
    "\n",
    "# 4. error statistics comparison\n",
    "ax4 = plt.subplot(2, 3, 4)\n",
    "stats = ['Mean', 'Median', '95th %ile']\n",
    "mpjpe_stats = [\n",
    "    results['joint_error_mm']['mean'],\n",
    "    results['joint_error_mm']['median'],\n",
    "    results['joint_error_mm']['p95'],\n",
    "]\n",
    "bars = ax4.bar(stats, mpjpe_stats, alpha=0.7, color='steelblue')\n",
    "ax4.set_ylabel('MPJPE (mm)')\n",
    "ax4.set_title('MPJPE Statistics')\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "# add value labels\n",
    "for bar, val in zip(bars, mpjpe_stats):\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{val:.2f}', ha='center', va='bottom')\n",
    "\n",
    "# 5. per-joint error heatmap (if we have per-joint data)\n",
    "ax5 = plt.subplot(2, 3, 5)\n",
    "# reshape per-joint errors for visualization\n",
    "per_joint_array = np.array(per_joint_means).reshape(-1, 1)\n",
    "im = ax5.imshow(per_joint_array, cmap='YlOrRd', aspect='auto')\n",
    "ax5.set_yticks(range(len(joint_names)))\n",
    "ax5.set_yticklabels(joint_names, fontsize=8)\n",
    "ax5.set_xticks([0])\n",
    "ax5.set_xticklabels(['Error (mm)'])\n",
    "ax5.set_title('Per-Joint Error Heatmap')\n",
    "plt.colorbar(im, ax=ax5)\n",
    "\n",
    "# 6. cumulative error distribution\n",
    "ax6 = plt.subplot(2, 3, 6)\n",
    "sorted_errors = np.sort(joint_errors)\n",
    "cumulative = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors)\n",
    "ax6.plot(sorted_errors, cumulative * 100, linewidth=2, color='steelblue')\n",
    "ax6.axvline(results['joint_error_mm']['mean'], color='r', linestyle='--', label=f\"Mean: {results['joint_error_mm']['mean']:.2f}mm\")\n",
    "ax6.axvline(results['joint_error_mm']['median'], color='g', linestyle='--', label=f\"Median: {results['joint_error_mm']['median']:.2f}mm\")\n",
    "ax6.set_xlabel('MPJPE (mm)')\n",
    "ax6.set_ylabel('Cumulative %')\n",
    "ax6.set_title('Cumulative Error Distribution')\n",
    "ax6.grid(True, alpha=0.3)\n",
    "ax6.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# save plots\n",
    "plot_path_drive = experiment_dir_drive / \"evaluation_plots.png\"\n",
    "plot_path_local = experiment_dir_local / \"evaluation_plots.png\"\n",
    "plt.savefig(plot_path_drive, dpi=150, bbox_inches='tight')\n",
    "shutil.copy(plot_path_drive, plot_path_local)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ“ evaluation plots saved:\")\n",
    "print(f\"  - google drive: {plot_path_drive}\")\n",
    "print(f\"  - local backup: {plot_path_local}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Access Checkpoints\n",
    "\n",
    "All checkpoints are automatically saved to Google Drive. Access them from:\n",
    "- **Google Drive**: `MyDrive/asha_checkpoints/{experiment_name}/`\n",
    "- **Colab file browser**: `/content/drive/MyDrive/asha_checkpoints/{experiment_name}/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint summary\n",
    "print(\"=\" * 60)\n",
    "print(\"CHECKPOINT FILES\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nðŸ“ Experiment: {experiment_name}\")\n",
    "print(f\"\\nðŸ’¾ Google Drive location:\")\n",
    "print(f\"   {experiment_dir_drive}\")\n",
    "print(f\"\\nðŸ“¦ Files saved:\")\n",
    "print(f\"   âœ“ best.pth: Best model checkpoint\")\n",
    "print(f\"   âœ“ latest.pth: Latest checkpoint\")\n",
    "print(f\"   âœ“ config.json: Training configuration\")\n",
    "print(f\"   âœ“ evaluation_results.json: Evaluation metrics\")\n",
    "print(f\"   âœ“ training_history.png: Training curves\")\n",
    "print(f\"\\nðŸ”— Access from:\")\n",
    "print(f\"   - Google Drive: MyDrive/asha_checkpoints/{experiment_name}/\")\n",
    "print(f\"   - Colab file browser: /content/drive/MyDrive/asha_checkpoints/{experiment_name}/\")\n",
    "print(f\"\\nðŸ’¡ Tip: Files are automatically synced to Drive. No manual download needed!\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. TensorBoard\n",
    "\n",
    "View training metrics in TensorBoard. Run this cell separately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorBoard extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Start TensorBoard (uncomment to run)\n",
    "# %tensorboard --logdir {CHECKPOINT_DIR} --port 6006\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
