# CIS 6800 Final Presentation - Complete Slide Guide

**Project**: Camera-Free Hand Pose Estimation via EMG and IK Pseudo-Labeling
**Duration**: 12-15 minutes
**Template**: Dark background (black/dark gray) with white text
**Format**: 13 slides + demo video

---

## üé® TEMPLATE STYLING NOTES

**Colors** (based on your template):
- Background: Black or very dark gray (#1a1a1a)
- Primary text: White (#ffffff)
- Accent colors for data:
  - Red/Salmon: #ff6b6b (for v4, worse results)
  - Teal/Cyan: #4ecdc4 (for v5, best results)
  - Green: #95e1d3 (for IK baseline)
  - Orange: #f38181 (for comparisons)
  - Yellow: #ffd93d (for highlights)

**Typography**:
- Title font: Bold, 40-48pt
- Body text: 20-24pt
- Captions: 16-18pt

**Layout**:
- Clean, minimal design
- High contrast (white on dark)
- Large figures (use full width when possible)
- Bullet points: 3-5 max per slide

---

## üìä SLIDE-BY-SLIDE BREAKDOWN

---

### **SLIDE 1: TITLE SLIDE** (30 seconds)

**Layout**: Centered text with subtle visual element

**Content**:
```
[Large, centered]
Camera-Free Hand Pose Estimation
via EMG and IK Pseudo-Labeling

[Medium, centered]
CIS 6800 Final Project

[Small, bottom]
Jeevan Karandikar
University of Pennsylvania
December 19, 2025
```

**Visual Element** (optional):
- Faint watermark: One of your hand mesh renders from docs/figures/v1_mano.png (set opacity to 15-20%)
- Position: Bottom right corner

**Speaker Notes**:
> "Today I'll present a system for camera-free hand tracking using muscle signals. The key innovation: using inverse kinematics to generate training labels, eliminating the need for $100K+ motion capture systems."

---

### **SLIDE 2: THE PROBLEM** (1 minute)

**Layout**: Two-column (60% text left, 40% visual right)

**Left Column - Text**:
```
THE PROBLEM

Why Camera-Free Tracking?

Vision-based systems fail when:
‚Ä¢ Hands are occluded by objects
‚Ä¢ Poor lighting conditions
‚Ä¢ Privacy concerns (visual recording)
‚Ä¢ High computational cost (5-10 fps)

EMG offers an alternative:
‚Üí Measures muscle activation directly
‚Üí Works regardless of occlusion
‚Üí Camera-free operation
```

**Right Column - Visual**:
Use `docs/figures/v0_mediapipe.png` or create a simple diagram showing:
- Top: Camera + hand with object (X mark)
- Bottom: EMG armband + hand (‚úì mark)

**Speaker Notes**:
> "Traditional hand tracking uses cameras. But cameras fail when hands are occluded by objects, struggle in poor lighting, and raise privacy concerns. EMG measures muscle signals directly - it works regardless of what a camera sees. But there's a catch: training EMG models requires expensive ground truth."

---

### **SLIDE 3: THE COST BARRIER** (45 seconds)

**Layout**: Split comparison with dramatic numbers

**Content**:
```
[Left side - RED background accent]
TRADITIONAL APPROACH
$100,000+
Motion Capture Lab
‚Ä¢ 26+ cameras
‚Ä¢ Controlled environment
‚Ä¢ Expert calibration
‚Ä¢ Hours of recording needed

[Right side - GREEN background accent]
OUR APPROACH
~$600
Laptop + EMG Armband
‚Ä¢ Single webcam
‚Ä¢ Any environment
‚Ä¢ Automatic IK optimization
‚Ä¢ 20 minutes of data
```

**Bottom (centered, bold)**:
```
Key Question: Can we generate reliable labels without mocap?
```

**Speaker Notes**:
> "Training EMG models traditionally requires motion capture systems costing over $100K. We asked: can we use inverse kinematics on webcam video to generate high-quality pseudo-labels instead? This would democratize EMG research."

---

### **SLIDE 4: OUR TWO-STAGE APPROACH** (1 minute)

**Layout**: Flow diagram with two stages

**Content**:
```
OUR SOLUTION: IK PSEUDO-LABELING PIPELINE

[Stage 1: Generate Labels]
Webcam Video
    ‚Üì
MediaPipe (21 keypoints)
    ‚Üì
MANO IK Optimization
    ‚Üì
9.71mm pseudo-labels

[Stage 2: Train EMG Model]
EMG Signals (8ch @ 500Hz)
    ‚Üì
Conv1D + LSTM
    ‚Üì
Direct Joint Prediction
    ‚Üì
14.92mm MPJPE @ 30fps
```

**Visual**: Use `docs/figures/version_timeline.png` if it shows the pipeline, or create simple arrow flow

**Bottom highlight box**:
```
Key Insight: Good-enough labels (9.71mm)
enable strong EMG models (14.92mm)
```

**Speaker Notes**:
> "Our approach has two stages. First, we use MediaPipe and MANO inverse kinematics to generate pseudo-labels at 9.71mm accuracy. These labels are good enough to train EMG models that achieve 14.92mm - competitive with vision-based systems, but completely camera-free."

---

### **SLIDE 5: MANO BACKGROUND** (45 seconds)

**Layout**: Image left (40%), explanation right (60%)

**Left Column**:
Image: `docs/figures/v1_mano.png` (MANO hand mesh render)

**Right Column - Text**:
```
MANO PARAMETRIC HAND MODEL

Representation:
‚Ä¢ 778 vertices (3D mesh)
‚Ä¢ 21 anatomical joints
‚Ä¢ Parametric control

Parameters:
Œ∏ (45 values) ‚Üí Joint angles
Œ≤ (10 values) ‚Üí Hand shape

Forward Kinematics:
Œ∏ ‚Üí Linear Blend Skinning ‚Üí Mesh

Inverse Kinematics:
21 keypoints ‚Üí Optimize Œ∏
```

**Speaker Notes**:
> "MANO is the industry-standard parametric hand model. It represents hands with 778 vertices controlled by 45 joint angles. Forward kinematics is fast: angles to mesh. Inverse kinematics is optimization: find angles that match observed keypoints. This is what we use to generate pseudo-labels."

---

### **SLIDE 6: MULTI-TERM IK OPTIMIZATION** (1.5 minutes)

**Layout**: Equation + explanation boxes

**Top - Main Equation** (centered, highlighted):
```
L_total = Œª_pos¬∑L_pos + Œª_dir¬∑L_dir + Œª_smooth¬∑L_smooth + Œª_reg¬∑L_reg
```

**Four boxes below** (2√ó2 grid):

```
[Box 1: Position]
L_pos = ||J_pred - J_target||¬≤
Align MANO joints
to MediaPipe keypoints

[Box 2: Bone Direction ‚≠ê]
L_dir = Œ£(1 - cos(vÃÇ_ij))
Scale-invariant
Anatomical realism

[Box 3: Temporal]
L_smooth = ||Œ∏_t - Œ∏_t-1||¬≤
Prevents jitter
Frame-to-frame consistency

[Box 4: Regularization]
L_reg = ||Œ∏||¬≤
Bias toward neutral pose
(Actually hurts - see Exp 1)
```

**Bottom** (settings box):
```
Optimizer: Adam (lr=0.01, 15 steps/frame)
Weights: Œª_pos=1.0, Œª_dir=0.5, Œª_smooth=0.1, Œª_reg=0.01
Alignment: Umeyama with scale estimation
```

**Speaker Notes**:
> "Our IK optimization uses four loss terms. Position alignment matches joints. Bone direction is critical - it's scale-invariant and enforces anatomical structure. Temporal smoothness prevents jitter. Regularization biases toward neutral poses. We'll see in our experiments that bone direction is essential, while regularization actually hurts performance."

---

### **SLIDE 7: EXPERIMENT 1 - LOSS ABLATION** (1.5 minutes)

**Layout**: Bar chart + table

**Top - Title**:
```
EXPERIMENT 1: Which Loss Terms Matter?
Dataset: 5,330 frames (3 min video)
```

**Main Visual**:
Use `docs/figures/exp1_loss_ablation.png` (bar chart showing MPJPE for each config)

**Key Table** (right side or below):
```
Configuration          MPJPE    Impact
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
All losses (baseline)  9.71 mm    ‚Äî
No bone direction     12.48 mm   +28% ‚ö†Ô∏è
No temporal            9.66 mm   -0.5%
No regularization      8.49 mm   -13% ‚úì
Position only          7.04 mm   Best numbers*
```

**Bottom callout box** (yellow highlight):
```
üîë KEY FINDING: Bone direction is CRITICAL (+28% error without it)
   Regularization actually hurts (-13% better without it)
   Position-only lacks anatomical guarantees
```

**Speaker Notes**:
> "Our first experiment tested which loss terms matter. Results: Bone direction is absolutely critical - removing it causes 28% worse error. Without it, the model can produce anatomically impossible poses. Surprisingly, regularization hurts performance. Position-only gets the best numbers but lacks structural guarantees, making it unsuitable for training labels. We use the full baseline config."

---

### **SLIDE 8: EXPERIMENTS 2-3 - ALIGNMENT & OPTIMIZER** (1 minute)

**Layout**: Two sections side-by-side

**Left Section**:
```
EXPERIMENT 2: ALIGNMENT METHOD

Method               MPJPE    Scale
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Umeyama scaled       9.71 mm  0.98¬±0.12 ‚úì
Umeyama rigid       10.31 mm  1.000
Kabsch              10.31 mm  1.000

Finding: Scale estimation
improves by 6%
```

Visual: `docs/figures/exp2_alignment.png` (if available)

**Right Section**:
```
EXPERIMENT 3: OPTIMIZER

Optimizer   MPJPE    Time    Conv.
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Adam        9.71 mm  56.7ms  99.7% ‚úì
L-BFGS     10.82 mm  38.8ms  99.3%
SGD        26.23 mm  55.0ms  92.6%

Finding: Adam best balance
of accuracy & reliability
```

Visual: `docs/figures/exp3_optimizer.png` (if available)

**Speaker Notes**:
> "Experiment 2 tested alignment methods. Umeyama with scale estimation performs best, handling MediaPipe's coordinate drift. Experiment 3 compared optimizers. Adam achieves 99.7% convergence with lowest error - only 16 failures in 5,330 frames. L-BFGS is faster but slightly less accurate. SGD fails on this non-convex optimization."

---

### **SLIDE 9: EXPERIMENT 4 - PER-JOINT ERROR** (1 minute)

**Layout**: Horizontal bar chart + hand diagram

**Top - Title**:
```
EXPERIMENT 4: Where Does IK Struggle?
```

**Main Visual**:
Use `docs/figures/exp4_per_joint.png` (horizontal bar chart showing error per joint)

**Key Insights Box** (right side):
```
WORST 5 JOINTS:

1. Thumb tip    14.13 mm
2. Wrist        13.13 mm
3. Index tip    11.85 mm
4. Thumb IP     11.41 mm
5. Pinky tip    11.32 mm

Mean: 9.71 mm across all 21 joints

Why fingertips fail:
‚Ä¢ Monocular depth ambiguity
‚Ä¢ Kinematic amplification
‚Ä¢ Small structures (thin)
```

**Speaker Notes**:
> "Experiment 4 analyzed per-joint error. Fingertips show 40-80% higher error due to monocular depth ambiguity - the camera can't distinguish depth well at the extremities. Wrist error comes from MANO-MediaPipe coordinate inconsistency. Despite these challenges, overall mean is 9.71mm - competitive with state-of-the-art vision systems."

---

### **SLIDE 10: IK VALIDATION SUMMARY** (30 seconds)

**Layout**: Results box with comparison

**Content**:
```
IK PSEUDO-LABEL QUALITY

Our IK System:  9.71 mm MPJPE @ 25 fps
                99.7% convergence rate
                3 min video (5,330 frames)

Comparison to State-of-the-Art:

Method              Dataset      MPJPE      FPS
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
HandFormer (SOTA)   STEREO      10.92 mm    ~5
HandFormer (SOTA)   FreiHAND    12.33 mm    ~5
Ours (IK)          Validation   9.71 mm     25 ‚úì

‚Üí Our IK labels are high-quality and suitable for EMG training
```

**Visual element**: Green checkmark or thumbs up icon

**Speaker Notes**:
> "Let's summarize the IK validation. Our system achieves 9.71mm, which beats HandFormer - a state-of-the-art vision transformer - on monocular setups. This validates that IK pseudo-labels are high quality enough for EMG training. No motion capture needed."

---

### **SLIDE 11: EMG TRAINING - TWO APPROACHES** (1.5 minutes)

**Layout**: Side-by-side comparison

**Top - Title**:
```
EXPERIMENT 5: EMG Model Architectures
Data: 20 min (5 sessions), 23,961 windows, 8ch @ 500Hz
Architecture: Conv1D (8‚Üí64‚Üí128) + 2-layer LSTM (256 hidden)
Training: A100 GPU, 100 epochs, ~1.5 hours
```

**Visual**:
Use `docs/figures/comparison_v4_vs_v5.png` (bar chart comparing v4 and v5)

**Two columns**:

```
[Left - v4: EMG ‚Üí MANO Œ∏]
Output: 45 pose parameters
Process: EMG ‚Üí Œ∏ ‚Üí MANO forward ‚Üí Joints
MPJPE: 34.92 mm
Params: 1.00M

Issue: Indirect supervision
MANO forward pass introduces error


[Right - v5: EMG ‚Üí Joints ‚≠ê]
Output: 63 values (21 joints √ó 3D)
Process: EMG ‚Üí Joints (direct)
MPJPE: 14.92 mm
Params: 1.01M

Advantage: Direct supervision
2.3√ó better than parametric!
```

**Bottom callout** (green highlight):
```
KEY FINDING: Direct joint prediction beats parametric by 2.3√ó
Simple, direct supervision wins over complex parametric encoding
```

**Speaker Notes**:
> "We tested two EMG architectures. v4 predicts MANO pose parameters, which then go through forward kinematics to get joints. v5 predicts joints directly. Both use the same Conv1D-LSTM architecture. Results: v5 achieves 14.92mm, beating v4's 34.92mm by 2.3√ó. Why? Direct supervision provides a clearer learning signal. The MANO forward pass in v4 amplifies EMG noise. Simplicity wins."

---

### **SLIDE 12: EXPERIMENT 6 - TRAINING CONVERGENCE** (1 minute)

**Layout**: Training curves + progression table

**Top - Title**:
```
EXPERIMENT 6: v5 Training Dynamics
```

**Main Visual**:
Use `docs/figures/v5_training.png` (training and validation loss curves)

**Progression Table** (right side):
```
TRAINING PROGRESSION

Stage           Epoch    Val MPJPE
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Initial            1     19.84 mm
Pre-LR drop       78     16.87 mm
Post-LR drop      99     14.92 mm

Total improvement: 4.92 mm (24.8%)
LR schedule benefit: 1.95 mm

Learning Rate Schedule:
‚Ä¢ Epochs 1-77:  lr = 1e-3
‚Ä¢ Epochs 78-99: lr = 5e-4
```

**Bottom insight box**:
```
üí° Key Insight: LR schedule crucial for final 1.95mm improvement
   Smooth convergence, no overfitting observed
```

**Speaker Notes**:
> "Experiment 6 analyzed training dynamics. The model started at 19.84mm and improved to 14.92mm over 100 epochs - a 25% improvement. The learning rate schedule was critical: dropping LR at epoch 78 gave us an extra 1.95mm improvement. Training was smooth with no overfitting, suggesting more data could push performance even further."

---

### **SLIDE 13: EXPERIMENT 7 - GROUND TRUTH QUALITY** (45 seconds)

**Layout**: Error breakdown diagram

**Top - Title**:
```
EXPERIMENT 7: Does IK Quality Limit EMG Performance?
```

**Content** (visual error breakdown):
```
ERROR ANALYSIS

IK Ground Truth Quality:    9.71 mm
EMG Model Performance:     14.92 mm
Gap:                        5.21 mm (54% overhead)

What contributes to the 5mm gap?
1. EMG model capacity
2. EMG signal noise
3. Subject-specific variability
4. IK label noise accumulation

[Visual: Stacked bar or flow diagram showing error accumulation]
```

**Insight Box** (bottom):
```
‚úÖ VALIDATION: Only 5mm gap proves IK pseudo-labels work!
   No expensive mocap needed for strong EMG models

Compare: Image‚ÜíPose models plateaued at 47mm
         (not viable for pseudo-labeling)
```

**Speaker Notes**:
> "Experiment 7 asked: does IK quality limit EMG performance? The 5mm gap between IK labels and EMG predictions is reasonable given we only used 20 minutes of data and EMG signals are noisy. This validates our core hypothesis: IK pseudo-labels are good enough. Compare this to image training, which plateaued at 47mm - completely unsuitable for generating training labels."

---

### **SLIDE 14: STATE-OF-THE-ART COMPARISON** (1 minute)

**Layout**: Comparison bar chart with callout

**Top - Title**:
```
HOW DO WE COMPARE TO PUBLISHED WORK?
```

**Main Visual**:
Use `docs/figures/comparison_state_of_art.png` (bar chart comparing methods)

**Comparison Table**:
```
Method              Dataset      MPJPE      Hardware       FPS
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
HandFormer (SOTA)   STEREO      10.92 mm   Stereo RGB     ~5
HandFormer (SOTA)   FreiHAND    12.33 mm   Mono RGB       ~5
Our IK System      Validation   9.71 mm    Mono RGB       25
Our EMG System     Validation  14.92 mm    EMG only ‚≠ê    30 ‚úì

Advantages of Our EMG System:
‚úì Camera-free (no occlusion/lighting issues)
‚úì Real-time (30 fps, 6√ó faster than transformers)
‚úì Affordable ($600 EMG vs $100K+ mocap training)
‚úì Privacy-preserving (no visual recording)
```

**Bottom callout** (large text, teal highlight):
```
14.92mm competitive with 12.33mm HandFormer
but COMPLETELY CAMERA-FREE üéØ
```

**Speaker Notes**:
> "How do we compare to state-of-the-art? Our IK system beats HandFormer on monocular setups. Our EMG system achieves 14.92mm - only 2mm worse than HandFormer on FreiHAND, but completely camera-free. This is competitive performance without any of the limitations of vision systems. It works in darkness, through occlusions, and at 6√ó the frame rate of vision transformers."

---

### **SLIDE 15: WHAT DIDN'T WORK** (1 minute)

**Layout**: Two failure boxes with lessons learned

**Top - Title**:
```
HONEST RESULTS: What We Tried First
```

**Two sections**:

```
[Left Box - RED accent]
‚ùå IMAGE ‚Üí POSE TRAINING

Approaches:
‚Ä¢ ResNet-50 + MLP
‚Ä¢ Transformer architecture
‚Ä¢ FreiHAND dataset (130K images)

Result: Plateaued at 47mm MPJPE

Why it failed:
‚Ä¢ Loss imbalance dominated training
‚Ä¢ Joint loss suppressed parameter learning
‚Ä¢ Model collapse (predicted average pose)
‚Ä¢ Not viable for pseudo-labeling (need <15mm)

Lesson: Architectural complexity
doesn't fix fundamental issues


[Right Box - RED accent]
‚ùå TRANSFER LEARNING

Approach:
‚Ä¢ Adapt Meta's emg2pose (16ch @ 2kHz)
‚Ä¢ ChannelAdapter: 8ch ‚Üí 16ch
‚Ä¢ FrequencyUpsampler: 500Hz ‚Üí 2kHz

Result: Complete failure

Why it failed:
‚Ä¢ Cannot hallucinate 8 missing electrodes
‚Ä¢ Cannot create 4√ó temporal resolution
‚Ä¢ Information fundamentally absent
‚Ä¢ Hardware mismatch is hard constraint

Lesson: Can't bridge hardware gaps
with learned mappings
```

**Bottom - Green box**:
```
‚úÖ WHAT WORKED: Direct training with simple architecture + high-quality IK labels
   Data quality > Architectural complexity
```

**Speaker Notes**:
> "Let's be honest about what didn't work. We tried image-to-pose training first - ResNet, Transformers - all plateaued at 47mm. Loss imbalance caused model collapse. Next, we tried transfer learning from Meta's emg2pose model. We built adapter networks to bridge from 8 to 16 channels and 500Hz to 2kHz. This failed completely. You cannot hallucinate missing electrodes or temporal resolution through learned mappings - the information isn't there. These failures taught us: simple architectures with high-quality labels beat overengineering."

---

### **SLIDE 16: SYSTEM EVOLUTION** (30 seconds)

**Layout**: Timeline visualization

**Content**:
```
PROJECT EVOLUTION: From Vision to Camera-Free

[Timeline with bars showing MPJPE]
```

Use `docs/figures/version_timeline.png`

**Key Milestones**:
```
v0: MediaPipe Baseline       ~17mm    @ 60fps  (Webcam)
v1: MANO IK Pseudo-Labels    9.71mm   @ 25fps  (Webcam) ‚úì
v2: EMG Integration          ‚Äî        @ 30fps  (Recording)
v4: EMG ‚Üí MANO Œ∏            34.92mm   @ 30fps  (EMG only)
v5: EMG ‚Üí Joints            14.92mm   @ 30fps  (EMG only) ‚≠ê
```

**Bottom text**:
```
Journey: 76 days (Oct 5 ‚Üí Dec 19)
Cost: ~$25 GPU compute + $600 hardware
Result: Camera-free tracking competitive with vision SOTA
```

**Speaker Notes**:
> "Here's our system evolution. We started with MediaPipe as a baseline, developed IK optimization to 9.71mm, integrated EMG recording, and trained models. v5 with direct joint prediction achieved 14.92mm - our best result. Total project time: 76 days. Total compute cost: about $25. The result: camera-free hand tracking competitive with state-of-the-art vision systems."

---

### **SLIDE 17: LIMITATIONS & FUTURE WORK** (1 minute)

**Layout**: Two columns

**Left Column - Current Limitations**:
```
LIMITATIONS

üî¥ Single user
   No cross-subject testing yet

üî¥ Limited training data
   20 min vs emg2pose's 370 hours

üî¥ Electrode drift
   Long sessions need recalibration

üî¥ Rest pose diversity
   Occasional false predictions
   during rest
```

**Right Column - Future Directions**:
```
FUTURE WORK

‚Üí Multi-user dataset
  5-10 subjects, 60+ min each
  Pre-train + fine-tune approach

‚Üí Extended recording
  Varied protocols, rest poses
  Rapid movements, sustained holds

‚Üí IMU fusion
  Wrist-mounted IMU for
  global hand orientation

‚Üí Real-world applications
  Prosthetic control
  AR/VR gesture input
  Accessibility interfaces
```

**Speaker Notes**:
> "Current limitations: single user, limited data, electrode drift. But these are addressable. Future work includes multi-user training with 5-10 subjects, extended recording sessions with more diverse gestures, IMU fusion for wrist orientation, and real-world applications like prosthetic control and AR/VR interaction."

---

### **SLIDE 18: CONCLUSIONS** (1 minute)

**Layout**: Key contributions + impact

**Top - Title**:
```
CONCLUSIONS
```

**Content**:
```
KEY CONTRIBUTIONS

1. IK Pseudo-Labeling Validated
   ‚Üí 9.71mm labels without $100K+ mocap
   ‚Üí 4 systematic ablation studies
   ‚Üí Beats HandFormer on monocular setup

2. Camera-Free Hand Tracking
   ‚Üí 14.92mm MPJPE at 30 fps
   ‚Üí Only 5mm worse than IK ground truth
   ‚Üí Works through occlusions, darkness

3. Direct > Parametric
   ‚Üí Simple joint prediction 2.3√ó better
   ‚Üí Architectural simplicity wins
   ‚Üí Data quality matters most

4. Hardware Constraints Matter
   ‚Üí Transfer learning failed (8ch vs 16ch)
   ‚Üí Documented failure helps community
   ‚Üí Direct training with quality labels works


BROADER IMPACT
‚Ä¢ Democratizes EMG research (no mocap needed)
‚Ä¢ Enables at-home data collection
‚Ä¢ Privacy-preserving interaction (no cameras)
‚Ä¢ Accessible prosthetic research (~$600 vs $100K)
```

**Bottom - Key Takeaway** (large text, highlighted):
```
Good-enough labels (9.71mm IK) + simple architecture
= competitive camera-free tracking (14.92mm)
```

**Speaker Notes**:
> "To conclude: We validated IK pseudo-labeling at 9.71mm through systematic experiments. We achieved camera-free tracking at 14.92mm, competitive with vision transformers. We showed direct joint prediction beats parametric approaches by 2.3√ó. And we documented why transfer learning failed - hardware compatibility is a hard constraint. The broader impact: this work democratizes EMG research, making it accessible without expensive mocap systems. Good-enough labels plus simple architectures can achieve competitive results."

---

### **SLIDE 19: THANK YOU / QUESTIONS** (Final slide)

**Layout**: Centered text with contact info

**Content**:
```
THANK YOU

Questions?

[Contact Info]
Jeevan Karandikar
jeev@seas.upenn.edu

[Optional: QR code to GitHub repo or project page]

[Bottom]
CIS 6800 - Computer Vision
University of Pennsylvania
December 2025
```

**Visual Element**:
- Small inset: Best frame from mano.py demo (hand mesh render)
- Or: Version timeline graphic

**Speaker Notes**:
> "Thank you. I'm happy to take questions about the methodology, results, or future directions."

---

## üé¨ DEMO VIDEO SCRIPT (2-3 minutes)

### **Demo Setup**
**Program**: `mano.py` (MANO IK pseudo-labeling system)
**Why this demo**: Shows the innovation that makes everything work (9.71mm IK labels)
**Recording**: Split-screen (webcam left, 3D mesh right) or picture-in-picture

---

### **Recording Instructions**

```bash
# 1. Terminal setup
cd /Users/jeevankarandikar/Documents/GitHub/asha
source asha_env/bin/activate
python -m asha.v1.mano

# 2. Recording software options:
# - QuickTime Player: File ‚Üí New Screen Recording
# - OBS Studio (better quality): Add window capture source
# - Built-in: Cmd+Shift+5 ‚Üí Record Selected Portion

# 3. Recording settings:
# - Resolution: 1920x1080 preferred (or 1280x720 minimum)
# - Frame rate: 30fps
# - Audio: Record voiceover OR add narration in post
# - Format: MP4 (H.264 codec)

# 4. Window arrangement:
# - Maximize mano.py window for clean frame
# - Good lighting on your hand
# - Clear background behind hand
# - Camera positioned for side/front view
```

---

### **Segment 1: Introduction** (15 seconds)

**Voiceover**:
> "This is our MANO inverse kinematics system - the core innovation that generates training labels for EMG models."

**Actions**:
- Show the GUI opening
- Wave hand to show tracking initializing
- Display IK error metric on screen (~9-10mm)

**On-screen text overlay**:
```
MANO IK Pseudo-Labeling
Real-time: 25 fps
Accuracy: 9.71mm MPJPE
```

---

### **Segment 2: Basic Gestures** (45 seconds)

**Voiceover**:
> "Watch how the 3D MANO mesh accurately follows my hand movements. The system optimizes 45 joint angles in real-time to match MediaPipe keypoints."

**Actions** (perform slowly and deliberately):
1. **Open hand ‚Üí Fist** (3 seconds hold each)
2. **Peace sign** (V fingers, 3 seconds)
3. **Pointing gesture** (index finger extended, 3 seconds)
4. **OK sign / Pinch** (thumb-index touch, 3 seconds)
5. **Individual finger movements** (wiggle each finger, 5 seconds)
6. **Thumbs up** (3 seconds)

**Camera work**:
- Keep hand in center frame
- Slight rotation to show 3D tracking
- Show mesh matching from multiple angles

**On-screen text** (optional):
```
Multi-term loss:
‚úì Position alignment
‚úì Bone direction (scale-invariant)
‚úì Temporal smoothness
```

---

### **Segment 3: Complex Movements** (30 seconds)

**Voiceover**:
> "The system handles complex poses with 99.7% convergence rate. Only 16 frames failed out of 5,330 in our validation set."

**Actions**:
1. **Rapid open/close** (fast fist pumps, 5 reps)
2. **Finger wave** (sequential finger movements, 5 seconds)
3. **Hand rotation** (show different angles, 5 seconds)
4. **Combined gesture** (complex multi-finger pose, hold 3 seconds)

**Highlight**:
- Point to error metric staying low (< 12mm)
- Show smooth tracking without jitter

---

### **Segment 4: Why This Matters** (30 seconds)

**Voiceover**:
> "This 9.71mm accuracy is better than HandFormer, a state-of-the-art vision transformer. These high-quality pseudo-labels enabled us to train EMG models to 14.92mm without any motion capture equipment."

**Actions**:
- Slow, smooth movements showing stable tracking
- Final gesture: open hand ‚Üí close to fist (hold)

**On-screen text overlays** (animate in sequence):
```
IK Labels: 9.71mm
vs HandFormer: 12.33mm

‚Üí Used to train EMG models
‚Üí No $100K mocap needed
‚Üí 20 min data sufficient

Final EMG Result: 14.92mm
100% camera-free ‚úì
```

---

### **Segment 5: Closing** (15 seconds)

**Voiceover**:
> "By generating reliable labels from inverse kinematics, we've demonstrated that camera-free EMG hand tracking is achievable without expensive infrastructure."

**Actions**:
- Wave goodbye gesture
- Freeze frame on clean mesh render

**Final on-screen text**:
```
Camera-Free Hand Tracking
IK Pseudo-Labeling: 9.71mm
EMG Model: 14.92mm @ 30fps

Accessible. Real-time. Robust.
```

---

### **Alternative: Quick Demo Script** (If pressed for time - 90 seconds)

**Condensed version**:

```
Segment 1 (20s): Introduction + setup
Segment 2 (40s): Key gestures (open, fist, peace, point, pinch)
Segment 3 (20s): Why it matters (comparison to SOTA)
Segment 4 (10s): Final message + freeze frame
```

---

### **Post-Production Tips**

1. **Add title card at start** (5 seconds):
   ```
   MANO IK Demonstration
   Real-Time Pseudo-Label Generation
   ```

2. **Overlay key metrics**:
   - Top corner: FPS counter, IK error value
   - Bottom: Current gesture label ("Open Hand", "Fist", etc.)

3. **Optional split-screen**:
   - Left 40%: Webcam feed
   - Right 60%: 3D mesh render
   - Shows input ‚Üí output mapping

4. **Background music** (optional):
   - Subtle, non-distracting
   - Low volume under voiceover
   - Upbeat, tech-oriented track

5. **Export settings**:
   - Resolution: 1920√ó1080 (or 1280√ó720)
   - Frame rate: 30fps
   - Bitrate: 5-10 Mbps
   - Format: MP4 (H.264)
   - Max file size: <100MB for easy sharing

---

## üìã PRE-PRESENTATION CHECKLIST

### **Content Preparation**

- [ ] All 19 slides created in Google Slides
- [ ] Dark template applied consistently
- [ ] All figures exported from `docs/figures/` and inserted
- [ ] Text copy-pasted exactly as specified
- [ ] Font sizes: Title 40-48pt, Body 20-24pt, Captions 16-18pt
- [ ] Color scheme applied (red/teal/green for data visualization)

### **Demo Video**

- [ ] `mano.py` tested and running smoothly
- [ ] Good lighting setup for hand visibility
- [ ] Screen recording software configured
- [ ] 2-3 minute demo recorded (all segments)
- [ ] Voiceover recorded OR script ready for live narration
- [ ] On-screen text overlays added (optional but recommended)
- [ ] Video edited and exported (MP4, <100MB)
- [ ] Demo video tested on presentation machine

### **Practice & Timing**

- [ ] Full presentation practiced 2-3 times with timer
- [ ] Each slide timing verified (1 min average, adjust as needed)
- [ ] Total time: 12-15 minutes confirmed
- [ ] Transitions smooth and natural
- [ ] Demo video integrates seamlessly (after Slide 16 or 18)

### **Q&A Preparation**

Anticipate these questions:

**Q1: "How would this work across different users?"**
> A: "Current model is single-user. For cross-subject generalization, I'd collect data from 5-10 users, pre-train a base model, then fine-tune per user with 5-10 minutes of calibration data. EMG signals vary across individuals due to anatomy, so some personalization is expected."

**Q2: "Why not just use cameras? They work pretty well now."**
> A: "Cameras fail under occlusion - imagine using tools, holding objects, or interacting with AR interfaces where your hands are frequently blocked. EMG works regardless. Plus, it's privacy-preserving (no visual recording) and computationally lighter than vision transformers."

**Q3: "Could you get better results with more data?"**
> A: "Absolutely. We only used 20 minutes. Meta's emg2pose used 370 hours. With 60-90 minutes of diverse data per user, I'd expect to get below 12mm, matching HandFormer. The convergence curves suggest we haven't saturated model capacity."

**Q4: "Why did transfer learning from emg2pose fail?"**
> A: "Hardware mismatch. Their system uses 16 channels @ 2kHz. Ours uses 8 channels @ 500Hz. You cannot hallucinate 8 missing electrode signals or 4√ó temporal resolution through learned mappings - the spatial and temporal information is fundamentally absent. Hardware compatibility is a hard constraint for sensor-based transfer learning."

**Q5: "Is this publishable?"**
> A: "In current form, it's workshop-level (CVPR Hands Workshop, CHI Late-Breaking Work). For main conference venues (UIST, ISMAR, ASSETS), I'd need: (1) multi-subject validation (10+ users), (2) cross-subject generalization tests, (3) longitudinal studies (does it work days/weeks later?), (4) comparison to other EMG methods. The core idea - IK pseudo-labeling - is novel and the results are competitive."

**Q6: "What about real-time constraints for prosthetic control?"**
> A: "Current inference is ~10ms per prediction (30fps system rate). For prosthetic control, you'd need <50ms end-to-end latency for natural feel. We're well within that. EMG preprocessing adds ~20ms, so total latency is ~30ms, which is acceptable for real-time control."

**Q7: "How robust is this to electrode shift/drift?"**
> A: "Not fully tested. Electrode impedance changes over long sessions (30+ min) can degrade performance. Future work includes online adaptation - using a small buffer of recent predictions to fine-tune normalization statistics in real-time. This is a known challenge in EMG research."

### **Technical Backup**

- [ ] PDF backup of slides on USB drive
- [ ] Demo video file on USB drive (in case WiFi/cloud fails)
- [ ] Laptop fully charged
- [ ] Presentation remote/clicker tested (if applicable)
- [ ] HDMI/display adapters ready
- [ ] Backup: Screenshots of key slides (in case slides don't load)

### **Day-Of**

- [ ] Arrive 10 minutes early to test A/V setup
- [ ] Test slides on presentation computer
- [ ] Test demo video playback
- [ ] Verify resolution/aspect ratio looks correct
- [ ] Water bottle ready (for 15-min talk)
- [ ] Backup: PDF on phone (last resort)

---

## üé§ PRESENTATION DELIVERY TIPS

### **General Advice**

1. **Pacing**: Aim for 1 minute per slide. You have 13-19 slides ‚Üí 12-15 min total.

2. **Emphasis**: Stress these key points:
   - "9.71mm IK beats HandFormer" (Slide 10)
   - "Bone direction is CRITICAL" (Slide 7)
   - "Direct joints 2.3√ó better than parametric" (Slide 11)
   - "Only 5mm gap validates IK pseudo-labeling" (Slide 13)
   - "Camera-free at 14.92mm competitive with 12.33mm HandFormer" (Slide 14)

3. **Honesty about failures**: Don't apologize, but be direct:
   - "Image training failed at 47mm, so we pivoted"
   - "Transfer learning taught us hardware compatibility matters"

4. **Confidence**: Your results are strong. 14.92mm is only 2mm worse than SOTA, but you're camera-free. That's impressive.

5. **Visual engagement**: Point to specific bars/numbers on charts as you discuss them.

6. **Transitions**: Use phrases like:
   - "Let's validate this with experiments..."
   - "Now that we have labels, let's train EMG models..."
   - "How does this compare to published work?"

### **Handling Technical Questions**

- **Don't fake it**: If you don't know, say "That's a great question - I'd need to investigate X to answer properly."
- **Redirect to results**: "While I haven't tested that, our experiments show..."
- **Future work**: "That's exactly what I'd explore next in [future work area]."

### **Body Language**

- **Stand confidently**: Don't pace or fidget
- **Face the audience**: Don't read from slides (use speaker notes)
- **Gesture naturally**: Point to slides, use hands to emphasize
- **Eye contact**: Look at audience (or camera if virtual), not screen

### **Time Management**

- **12-min version**: Skip Slide 16 (System Evolution) or merge Slides 8-9
- **15-min version**: Full 19 slides + demo video
- **Built-in buffer**: Slides 16-17 (Evolution, Limitations) can be shortened if running long

---

## üìä FIGURES CHECKLIST

### **Figures You Have** (from `docs/figures/`)

‚úÖ Available:
- `exp1_loss_ablation.png` (Slide 7)
- `exp2_alignment.png` (Slide 8)
- `exp3_optimizer.png` (Slide 8)
- `exp4_per_joint.png` (Slide 9)
- `comparison_v4_vs_v5.png` (Slide 11)
- `v5_training.png` (Slide 12)
- `comparison_state_of_art.png` (Slide 14)
- `version_timeline.png` (Slide 16)
- `v0_mediapipe.png`, `v1_mano.png`, `v2_emg.png` (various)

### **Figures You May Need to Create**

If any figures are missing, create simple versions:

1. **Problem Statement (Slide 2)**:
   - Use `v0_mediapipe.png` or create simple diagram:
     - Top: Camera + occluded hand (red X)
     - Bottom: EMG armband (green check)

2. **Pipeline Diagram (Slide 4)**:
   - Simple arrow flowchart:
     ```
     Webcam ‚Üí MediaPipe ‚Üí MANO IK ‚Üí 9.71mm
     EMG ‚Üí Conv1D+LSTM ‚Üí Joints ‚Üí 14.92mm
     ```

3. **MANO Mesh (Slide 5)**:
   - Use `v1_mano.png` (screenshot from mano.py)

4. **Cost Comparison (Slide 3)**:
   - Simple text-based comparison (shown in slide content)

---

## ‚úÖ FINAL RECOMMENDATIONS

### **Use These Slides** (13-slide version for 12-15 min):

1. Title
2. The Problem
3. The Cost Barrier
4. Our Two-Stage Approach
5. MANO Background
6. Multi-Term IK Optimization
7. Experiment 1 - Loss Ablation ‚≠ê
8. Experiments 2-3 - Alignment & Optimizer
9. Experiment 4 - Per-Joint Error
10. IK Validation Summary
11. EMG Training - Two Approaches ‚≠ê
12. Experiment 6 - Training Convergence
13. Experiment 7 - Ground Truth Quality
14. State-of-the-Art Comparison ‚≠ê
15. What Didn't Work ‚≠ê
16. System Evolution
17. Limitations & Future Work
18. Conclusions ‚≠ê
19. Thank You / Questions

**Starred slides (‚≠ê)**: Most important, spend extra time here

### **Demo Video Placement**

**Option A** (Recommended):
- After Slide 10 (IK Validation Summary)
- Transition: "Let me show you the IK system in action..."
- 2-3 minute video
- Resume with Slide 11 (EMG Training)

**Option B**:
- After Slide 18 (Conclusions)
- Transition: "Here's our pseudo-labeling system generating training data..."
- 2 minute video
- End with Slide 19 (Thank You)

### **Timing Breakdown** (15-minute version)

| Section | Slides | Time |
|---------|--------|------|
| Introduction | 1-4 | 3:15 |
| MANO & IK | 5-6 | 2:15 |
| IK Experiments | 7-10 | 4:00 |
| **[Demo Video]** | - | 2:30 |
| EMG Training | 11-13 | 3:30 |
| Results & Conclusion | 14-19 | 4:30 |
| **Total** | **19 slides + demo** | **15:00** |

---

## üöÄ YOU'RE READY!

**Your project has**:
‚úÖ Strong technical contribution (IK pseudo-labeling)
‚úÖ Systematic validation (7 experiments)
‚úÖ Competitive results (14.92mm vs 12.33mm SOTA)
‚úÖ Practical impact (democratizes EMG research)
‚úÖ Honest failures (image training, transfer learning)
‚úÖ Real-time demo (mano.py)

**Presentation strengths**:
- Clear narrative arc (problem ‚Üí solution ‚Üí validation ‚Üí results)
- Data-driven (every claim backed by experiments)
- Visually clean (dark template, high-contrast figures)
- Honest about limitations (builds credibility)
- Actionable future work (shows you understand next steps)

**Your key message**:
> "We proved you don't need $100K mocap for EMG hand tracking. High-quality inverse kinematics pseudo-labels (9.71mm) enable camera-free tracking (14.92mm) that's competitive with state-of-the-art vision systems, using just 20 minutes of self-collected data."

---

**Good luck with your presentation! You've done excellent work. üéâ**
